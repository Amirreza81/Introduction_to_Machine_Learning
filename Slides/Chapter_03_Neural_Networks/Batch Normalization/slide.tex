%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Beamer template for University of Wollongong     %
% Based on THU beamer theme                          %
% Author: Qiuyu Lu                                   %
% Date: July 2024                                    %
% LPPL Licensed.                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Customized for Sharif University of Technology     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[serif, aspectratio=169]{beamer}
\usepackage{pgfplots} % Required for plotting
\pgfplotsset{compat=1.17} % Compatibility level
%\documentclass[serif]{beamer}  % for 4:3 ratio
\usepackage[T1]{fontenc} 
\usepackage{fourier} % see "http://faq.ktug.org/wiki/uploads/MathFonts.pdf" for other options
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{subfigure}
\usetikzlibrary{positioning, arrows.meta}

\author{Ali Sharifi-Zarchi}
\title{Machine Learning (CE 40477)}
\subtitle{Fall 2024}
\institute{
    CE Department \\
    Sharif University of Technology
}
%\date{\small \today}
% \usepackage{UoWstyle}
\usepackage{SUTstyle}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}


\begin{document}

\begin{frame}
    \titlepage
    \vspace*{-0.6cm}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/sharif-main-logo.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}


\section{Batch Normalization}
\subsection{Batch Normalization introduction}
\begin{frame}{What is Batch Normalization Concept?}

\begin{itemize}
   \item Batch Normalization main purpose: \textcolor{blue}{\textbf{Smoothing the optimization space}}
   \item Batch Normalization Opt.: Normalizing activations in a network.
\end{itemize}

\end{frame}

\begin{frame}{Smoothing the optimization space}

        \begin{figure}
        \includegraphics[width=0.85\textwidth]{pic/BatchNorm_Effect.png}
        \caption{Using Batch Normalization causes the optimization space to become smoother. \href{https://www.linkedin.com/pulse/ways-improve-your-deep-learning-model-batch-adam-albuquerque-lima}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:BN_Effect1}
    \end{figure}

\end{frame}

\subsection{Why Batch Normalization?}

\begin{frame}{Why Batch Normalization?}

    \textcolor{blue}{\textbf{Problem: Internal Covariate Shift}}

\begin{itemize}

    \item Wait! What does it mean, in simple language?
    \item Let’s say that we want to train a model and the ideal target output function that the model needs to learn is as below:

    \begin{figure}
        \includegraphics[width=0.45\textwidth]{pic/ICS-1.png}
        \caption{Target function \href{https://ketanhdoshi.github.io/assets/images/BatchNorm/ICS-1.png}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:Target_function}
    \end{figure}

\end{itemize}

\end{frame}

\begin{frame}{Problem: Internal Covariate Shift}

\begin{itemize}

    \item Suppose that the training data values input to the model cover only a part of the range of output values. Therefore, the model can only learn a subset of the target function.

    \begin{figure}
        \includegraphics[width=0.45\textwidth]{pic/ICS-2.png}
        \caption{Training data distribution \href{https://ketanhdoshi.github.io/assets/images/BatchNorm/ICS-2.png}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:Training_data_distribution}
    \end{figure}

\end{itemize}

\end{frame}

\begin{frame}{Problem: Internal Covariate Shift}

\begin{itemize}

    \item The model has no idea about the rest of the target curve. It could be anything.

    \begin{figure}
        \includegraphics[width=0.45\textwidth]{pic/ICS-3.png}
        \caption{Rest of the target curve \href{https://ketanhdoshi.github.io/assets/images/BatchNorm/ICS-3.png}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:Rest_Curve}
    \end{figure}

\end{itemize}

\end{frame}

\begin{frame}{Problem: Internal Covariate Shift}

\begin{itemize}

    \item Suppose we feed the model to the testing data as below.
    \item This has a very different distribution from the data that the model was initially trained with.
    \item The model cannot simply generalize its predictions for this new data.

    \begin{figure}
        \includegraphics[width=0.4\textwidth]{pic/ICS-4.png}
        \caption{The rest of the data has different distribution \href{https://ketanhdoshi.github.io/assets/images/BatchNorm/ICS-4.png}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:Rest_data_distribution}
    \end{figure}

\end{itemize}

\end{frame}

\begin{frame}{Problem: Internal Covariate Shift}
    
\begin{itemize}
    \item This is the problem of Covariate Shift \textcolor{green}{the model is fed data with a different distribution than what it was previously trained with}, even though that new data still conforms to the same target function.

    \item For the model to figure out how to adapt to this new data, it has to re-learn some of its target output functions.
    \item \textbf{This slows down the training process.}
    
\end{itemize}

\end{frame}

\begin{frame}{Problem: Internal Covariate Shift}

\textcolor{blue}{\textbf{What happens inside Deep Network Layers?}}

\begin{itemize}

\item Think about what happens when we train a deep network: As we update the weights of earlier layers, the data distribution in the deeper layers keeps shifting.
\item This means that deeper layers see new and varying patterns every time we update the weights in previous layers.
\item As a result, the network must keep readjusting, which makes the learning process slower and more difficult.
\item \textbf{In other words:} The network is constantly “re-learning” how to make predictions because the data it sees is never consistent!

\end{itemize}

\end{frame}

\begin{frame}{Batch Normalization Solution}

    \begin{itemize}
        \item \textbf{Goal:} Normalize inputs so that the mean is near 0 and the variance is close to 1.
        \item \textbf{How it helps:} Stabilizes learning, allowing for higher learning rates and faster convergence.
    \end{itemize}

\end{frame}

\begin{frame}{Magic Effect of Batch Normalization}

    \textcolor{blue}{\textbf{Magic Effect!}}
    
    \begin{itemize}
            \item Batch Normalization helps the network train faster and achieve higher accuracy.
            \item Batch Normalization \textbf{makes the distribution more stable} and reduces the internal covariate shift.
    \end{itemize}

    \begin{figure}
        \includegraphics[width=0.7\textwidth]{pic/BN.png}
        \caption{(a) The test accuracy of the MNIST network trained with and without Batch Normalization, vs the number of training steps. (b, c) The evolution of input distributions to a typical sigmoid, for training, shown as {15, 50, 85}th percentiles [1].}
        \label{fig:BN_Effect}
    \end{figure}
\end{frame}


\subsection{How Batch Normalization Works}

\begin{frame}{How Batch Normalization Works}

    \textcolor{blue}{\textbf{Process Overview}}
    
    \begin{itemize}

        \item  For each mini-batch during training, batch normalization normalizes the inputs to a layer by adjusting their mean and variance.

    \end{itemize}
\end{frame}

% Frame 1: Steps 1 and 2
\begin{frame}{How Batch Normalization Works}
    \textcolor{blue}{\textbf{Steps in Batch Normalization}}
    
    \begin{enumerate}[<+-| alert@+>] % stepwise alerts
        \item \textbf{Compute the Mean and Variance} 
              \newline
              For a given mini-batch, compute the mean $\mu_B$ and variance $\sigma_B^2$ of the inputs:
    
              \[
               \mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i, \quad \sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2
              \]
        \item \textbf{Normalize the Inputs}
        \newline
        Subtract the mean and divide by the standard deviation to get normalized activations:

        \[
        \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
        \]

        where $\epsilon$ is a small constant added for numerical stability.
    \end{enumerate}
\end{frame}

% Frame 2: Step 3
\begin{frame}{How Batch Normalization Works (Continued)}
    \textcolor{blue}{\textbf{Steps in Batch Normalization}}
    
    % Resume the enumeration from step 3
    \begin{enumerate}[<+-| alert@+>] % stepwise alerts
        \setcounter{enumi}{2} % Set the counter to continue from step 3
        \item \textbf{Scale and Shift}
        \newline
        After normalization, introduce learnable parameters $\gamma$ and $\beta$ that allow the model to scale and shift the normalized output:
    
        \[
            y_i = \gamma \hat{x}_i + \beta
        \]

        This ensures that the model can recover the original data distribution if needed.
    \end{enumerate}
\end{frame}

\begin{frame}{How Batch Normalization Works}

    \textcolor{blue}{\textbf{Inference Mode:}} {\textbf{\textcolor{red}{Hint!!!}}}
    
    \begin{itemize}

        \item During inference (when predicting new data), batch statistics (mean and variance) are replaced with moving averages collected during training.

    \end{itemize}
\end{frame}


\begin{frame}{Effect of Batch Normalization on Gradients}
        \begin{figure}
        \includegraphics[width=0.9\textwidth]{pic/BN_Grad_Dist.png}
        \caption{Gradient magnitudes at initialization for layer 55 of a network with and without Batch Normalization. \href{https://doi.org/10.48550/arXiv.1806.02375}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:BN_Dist}
    \end{figure}
\end{frame}

\begin{frame}{Effect of Batch Normalization on Gradients}
        \begin{figure}
        \includegraphics[width=0.55\textwidth]{pic/Smooth.png}
        \caption{Gradients with Batch Norm are smoother. \href{https://ketanhdoshi.github.io/Batch-Norm-Why/}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:Smooth}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Effect of Batch Normalization on Gradients}

    The main benefit of batch normalization is that it reduces the dependency of the gradient on the scale of the input and parameters:

    \begin{equation}
        \frac{\partial \mathcal{L}}{\partial x} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial \hat{x}} \cdot \frac{\partial \hat{x}}{\partial x}
    \end{equation}

    Where:
    \begin{itemize}
        \item \centering \(\frac{\partial y}{\partial \hat{x}} = \gamma\) \\
        \item \(\frac{\partial \hat{x}}{\partial x} = \frac{1}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}\)
    \end{itemize}

    Thus, the gradient becomes:
    \begin{equation}
        \frac{\partial \mathcal{L}}{\partial x} = \frac{\gamma}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}} \cdot \frac{\partial \mathcal{L}}{\partial y}
    \end{equation}
\end{frame}


\begin{frame}
    \frametitle{Loss Landscape is not Smooth in Typical Neural Networks}
    \begin{figure}
        \includegraphics[width=0.45\textwidth]{pic/Landscape-1.png}
        \caption{A neural network loss-landscape. \href{https://arxiv.org/pdf/1712.09913.pdf}{\textcolor{orange}{\textbf{Source}}}}
        \label{fig:Loss_Landscape}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{How Batch Normalization Smooth the  Loss Landscape}

    The smoothing effect of batch normalization can be understood by observing how it constrains the gradient magnitudes. The expression shows that:
    \begin{equation}
        \frac{\partial \mathcal{L}}{\partial x} \text{ is scaled by } \frac{1}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}
    \end{equation}

    This consistent scaling leads to a smooth loss landscape because:
    \begin{itemize}
        \item It stabilizes the gradient flow, ensuring controlled optimization step sizes.
        \item Reduces the risk of large oscillations or abrupt changes in the loss landscape.
        \item Makes the optimization process less likely to be trapped in local minima or saddle points.
    \end{itemize}
\end{frame}


\subsection{Batch Normalization Pros \& Cons}

\begin{frame}{Batch Normalization Pros}

    \textcolor{green}{\textbf{Pros}}
    
    \begin{itemize}

        \item \textbf{Faster Convergence:}  Empirical results support that models with batch normalization converge faster and achieve higher accuracy, even with higher learning rates.
        \item \textbf{Reduced Sensitivity to  Weight Initialization:} Helps mitigate the dependency on careful weight initialization.
        \item \textbf{Acts as Regularization:} Batch normalization can help reduce overfitting.
        \item \textbf{Reduces Vanishing/Exploding Gradients:} Helps maintain stable gradients throughout deep networks.


    \end{itemize}
\end{frame}

\begin{frame}{Batch Normalization Pros}

    \textcolor{blue}{\textbf{Why Using Batch Normalization Reduces Sensitivity to Weight Initialization?}}

    \begin{figure}[h]
        \centering
        \subfigure[]{
            \includegraphics[width=0.3\textwidth]{pic/BN1.png}
            \label{fig:BN_Pros1}
        }
        \subfigure[]{
            \includegraphics[width=0.33\textwidth]{pic/BN2.png}
            \label{fig:BN_Pros2}
        }
        \caption{Start point doesn't matter! \href{https://www.linkedin.com/pulse/ways-improve-your-deep-learning-model-batch-adam-albuquerque-lima}{\textcolor{orange}{\textbf{Source}}}}
    \end{figure}
\end{frame}

\begin{frame}{Batch Normalization Pros}

    \textcolor{blue}{\textbf{Why Does Batch Normalization Reduce Sensitivity to Weight Initialization?}}
    \begin{itemize}
        \item Batch normalization smooths the optimization landscape, reducing the dependency on initial weights. 
        \item This allows the model to converge to a minimum efficiently, \textcolor{green}{regardless of where optimization begins}.
        


    \end{itemize}
\end{frame}

\begin{frame}{Batch Normalization Cons}

    \textcolor{red}{\textbf{Cons}}
    
    \begin{itemize}

        \item \textbf{Batch Size Sensitivity:} Performance can depend on batch size, and very small batches may not provide stable statistics.
        \item \textbf{Computational Overhead:} Adds extra computation during training.
        \item \textbf{Behavior During Inference:} The shift from batch statistics to moving averages during inference may lead to slight discrepancies.

    \end{itemize}
\end{frame}

\subsection{Batch Normalization in Practice}

% \begin{frame}{Batch Normalization in Practice}

%     \textcolor{blue}{\textbf{Where to Apply}}
    
%     \begin{itemize}

%         \item \textbf{Typical Location:} Apply after the linear transformation (e.g., after a dense or convolutional layer) but before the activation function
%         \item \textbf{Layer Placement:}
%             \newline
%             Dense layer -> Batch Normalization -> Activation function
%             Conv layer -> Batch Normalization -> Activation function

%     \end{itemize}
% \end{frame}

\begin{frame}{Batch Normalization in Practice}

    \textcolor{blue}{\textbf{Where to Apply}}
    
    \begin{itemize}
        \item \textbf{Typical Location:} Apply after the linear transformation (e.g., after a dense or convolutional layer) but before the activation function.
        \item \textbf{Layer Placement:}
            \newline
\begin{center}
    \begin{tikzpicture}[
        node distance=0.9cm and 1.5cm, % Distance between nodes
        every node/.style={draw, rounded corners, text centered, minimum height=1cm, minimum width=2.5cm}, % Style for every node
        bn/.style={fill=green!20}, % Style for batch normalization nodes
        conv/.style={fill=red!20}, % New style for conv layer
        dense/.style={fill=cyan!20}, % New style for dense layer
        act/.style={fill=yellow!20}, % New style for activation functions
        ->, >=Stealth % Arrow styles
    ]

        % Define nodes for the first path
        \node[dense] (dense) {Dense Layer};
        \node[bn] (bn1) [right=of dense] {Batch Normalization};
        \node[act] (act1) [right=of bn1] {Activation Function};

        % Define nodes for the second path
        \node[conv] (conv) [below=of dense] {Conv Layer};
        \node[bn] (bn2) [right=of conv] {Batch Normalization};
        \node[act] (act2) [right=of bn2] {Activation Function};

        % Draw arrows between nodes
        \draw[->] (dense) -- (bn1);
        \draw[->] (bn1) -- (act1);

        \draw[->] (conv) -- (bn2);
        \draw[->] (bn2) -- (act2);

    \end{tikzpicture}
\end{center}
    \end{itemize}
    
\end{frame}



\subsection{Closing Takeaways on Batch Normalization}

\begin{frame}{Batch Normalization in Practice}
    
    \begin{itemize}

    \item \textbf{Key Point:} Batch normalization stabilizes and accelerates training while providing regularization benefits.
    \item \textbf{Impact on Training:} Enables efficient training of deeper networks with reduced hyperparameter tuning.

\end{itemize}

\end{frame}


\section{References}

\begin{frame}[allowframebreaks]
    \bibliography{ref}
    \bibliographystyle{ieeetr}
    \nocite{*} % used here because no citation happens in slides
    % if there are too many try use：
    % \tiny\bibliographystyle{alpha}
\end{frame}


\begin{frame}
    \begin{center}
        {\Huge Any Questions?}
    \end{center}
\end{frame}

\end{document}