%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Beamer template for University of Wollongong     %
% Based on THU beamer theme                          %
% Author: Qiuyu Lu                                   %
% Date: July 2024                                    %
% LPPL Licensed.                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Customized for Sharif University of Technology     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[serif, aspectratio=169]{beamer}
%\documentclass[serif]{beamer}  % for 4:3 ratio
\usepackage[T1]{fontenc} 
\usepackage{fourier} % see "http://faq.ktug.org/wiki/uploads/MathFonts.pdf" for other options
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{lipsum}
\usepackage{booktabs}
%\usepackage{geometry}
\usepackage{tabularx}
\author{Ali Sharifi-Zarchi}
\title{Machine Learning (CE 40717)}
\subtitle{Fall 2024}
\institute{
    CE Department \\
    Sharif University of Technology
}
%\date{\small \today}
% \usepackage{UoWstyle}
\usepackage{SUTstyle}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}

\begin{frame}
    \titlepage
    \vspace*{-0.6cm}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/sharif-main-logo.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Optimization}

\begin{frame}{Optimization Problem}
    \begin{itemize}
        \item \textbf{Goal}: Find the value of $x$ where $f(x)$ is at a \textbf{minimum} or \textbf{maximum}.
        \item In neural networks, we aim to minimize \textbf{prediction error} by finding the optimal weights $w^*$:
    \end{itemize}
    
    \[
    w^* = \arg\min_{w} J(w)
    \]
    
    \begin{itemize}
        \item Simply put: determine the \textbf{direction to step} that will quickly \textbf{reduce loss}.
    \end{itemize}
    
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.3\linewidth]{pic/downhill_feifei_rouhan.png}
        \caption{Which way is downhill? (CS231n, Stanford)}
    \end{figure}
\end{frame}

\begin{frame}{Convexity and Optimization}
    \begin{itemize}
        \item \textbf{Convex Functions}:
        \begin{itemize}
            \item A function is \textbf{convex} if any line segment between points on the curve lies \textbf{above or on} the curve.
            \item Convex functions are easier to optimize, as they have a single \textbf{global minimum}.
            \item \textbf{Gradient Descent} is guaranteed to reach the global minimum in convex functions.
        \end{itemize}
    \end{itemize}
    \begin{figure}[h]
        \centering
        \includegraphics[height=0.5\textheight]{pic/loss_convex.jpg}
        \caption{Example of convex function (bowl shape)}
    \end{figure}
\end{frame}

\begin{frame}{Non-Convex Functions and Challenges}
\begin{minipage}{0.45\linewidth}
    \begin{itemize}
        \item \textbf{Non-Convex Functions}:
        \begin{itemize}
            \item Characterized by multiple \textbf{local minima} and \textbf{saddle points}.
            \item \textbf{Global Minimum}: Overall lowest point.
            \item \textbf{Local Minimum}: Lower than nearby points, but not the lowest overall.
            \item \textbf{Saddle Points}: Regions where the gradient is close to zero but can increase or decrease in other directions.
        \end{itemize}
        \item Finding the \textbf{global minimum} is more complex in non-convex functions.
    \end{itemize}
\end{minipage}%
\begin{minipage}{0.45\linewidth}
    \begin{figure}[h]
        \centering
        \includegraphics[height=0.6\textheight]{pic/convex_nonconvex.png}
        \caption{ Convex (top) vs. Non-Convex (bottom) functions (CMU, 11-785)}
    \end{figure}
\end{minipage}
\end{frame}

\section{The Loss Surface}

\begin{frame}{Loss Surface Definition}
    \begin{itemize}
        \item The \textbf{loss surface} shows how error changes based on network weights.
        \item For neural networks, the loss surface is typically \textbf{non-convex} due to multiple layers, nonlinear activations, and complex parameter interactions, resulting in \textbf{multiple local minima} and \textbf{saddle points}.
        \item In large networks, most local minima yield similar error values close to the \textbf{global minimum}; this is less true in smaller networks.
    \end{itemize}
    \begin{figure}[h]
        \centering
        \includegraphics[height=0.4\textheight]{pic/resnet56_noshort_small.jpg}
        \caption{\footnotesize Loss surface of ResNet56. Source: \url{https://github.com/tomgoldstein/loss-landscape}}
    \end{figure}
\end{frame}

\begin{frame}{Loss Optimization}
    \begin{itemize}
        \item \textbf{Goal}: How can we optimize a non-convex loss function effectively?
        \item \textbf{Newton's Method}:
        \begin{itemize}
            \item When optimizing, we look for critical points where the derivative \( f'(x) = 0 \), which may indicate minima, maxima, or saddle points.
            \item Newton's Method uses the second derivative (Hessian) to adjust step sizes, which can lead to faster convergence compared to Gradient Descent.
        \end{itemize}
        \item \textbf{Gradient Descent}:
        \begin{itemize}
            \item This method identifies the steepest descent direction to guide the optimization process.
        \end{itemize}
    \end{itemize}
    % \begin{figure}[h]
    %     \centering
    %     \includegraphics[height=0.4\textheight]{pic/gradient_descent_blog.skz.dev.png}
    %     \caption{\footnotesize Visualization of Gradient Descent. Source: \url{https://blog.skz.dev/gradient-descent}}
    % \end{figure}
\end{frame}

\section{Newton's Method}
\begin{frame}{Overview of Newton's Optimization Method}
\begin{minipage}{0.65\linewidth}
    \begin{itemize}
        \item \textbf{Origin}: 
        \begin{itemize}
            \item Developed from classical physics and calculus, Newton's method aims to optimize functions by leveraging \textbf{curvature} information.
            \item Named after Sir Isaac Newton, who introduced the idea of using \textbf{second derivatives} to predict the behavior of functions.
        \end{itemize}
        
        \item \textbf{Objective}:
        \begin{itemize}
            \item Directly find optimal weights by considering both gradients (first derivative) and curvature (second derivative) of the loss surface.
        \end{itemize}
    \end{itemize}
\end{minipage}%
\begin{minipage}{0.25\linewidth}
    \begin{figure}
        \centering
        \includegraphics[width=1\linewidth]{pic/sgd vs newton_wiki.png}
        \caption{A comparison of gradient descent (green) and Newton's method (red) for minimizing a function. Source: Wikipedia}
    \end{figure}
\end{minipage}
\end{frame}

\begin{frame}{Newton's Optimization Method: Mathematical Formulation}
    \begin{itemize}
        \item \textbf{Core Mechanism}:
        \begin{itemize}
            \item Newton's method adjusts weight updates using the Hessian matrix, incorporating second-order derivative information to account for local curvature.
        \end{itemize}

        \item \textbf{Update Rule}:
        \[
        w_{t+1} = w_t - H^{-1} \nabla_w J(w_t)
        \]
        where:
        \begin{itemize}
            \item \( H \) is the Hessian matrix of second-order partial derivatives, capturing curvature information.
            \item \( \nabla_w J(w_t) \) is the gradient of the cost function \( J \) at time \( t \).
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Newton's Method: Advantages and Disadvantages}
    \begin{itemize}
        \item Newton's method offers various benefits but also has limitations, especially in large-scale machine learning. Below is a summary:
    \end{itemize}
    \begin{table}[]
        \centering
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Advantages} & \textbf{Disadvantages} \\
            \hline
            \textbf{Faster Convergence} & \textbf{Computationally Expensive} \\
            Quadratic convergence enables reaching & Requires Hessian calculation, making it\\
            minima faster in convex problems. & costly in high-dimensional models. \\
            \hline
            \textbf{Adaptive Step Sizes} & \textbf{Memory Intensive} \\
            Curvature-based step adjustment avoids & Storing the Hessian matrix is memory-intensive\\
            slow progress in shallow regions. & for models with millions of parameters. \\
            \hline
            \textbf{Reduced Oscillations} & \textbf{Convergence Challenges} \\
            Curvature information stabilizes paths & May converge to saddle points in non-convex\\
            in oscillatory regions. & functions common in machine learning. \\
            \hline
        \end{tabular}
    \end{table}
\end{frame}


\section{Gradient Descent}
\begin{frame}{Gradient Descent Overview}
    \begin{itemize}
        \item \textbf{Gradient Descent}: As mentioned earlier in this course, Gradient Descent is an iterative method to minimize error by updating weights in the direction of the \textbf{negative gradient}:
        \[
        w_{t+1} = w_t - \eta \nabla J(w_t)
        \]
        where $\eta$ is the \textbf{learning rate}.
    \end{itemize}
    \begin{itemize}
        \item \textbf{Types of Gradient Descent}:
        \begin{itemize}
            \item \textbf{Batch}: Full dataset for stable but slow updates.
            \item \textbf{Stochastic (SGD)}: One data point for fast, noisy updates.
            \item \textbf{Mini-Batch}: Small batches, balancing speed and stability.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Problems with Gradient Descent}
    \begin{itemize}
        \item \textbf{High Variability (SGD)}: Quick in steep directions but slow in shallow ones, causing \textbf{jitter and slow progress}.
        \item \textbf{Local Minima and Saddle Points}: Risk of \textbf{sub-optimal solutions} or long convergence times in flat regions.
        \item \textbf{Noisy Updates}: Using individual points or mini-batches introduces noise, affecting stable convergence.
    \end{itemize}
    \vfill
    \begin{minipage}{0.5\linewidth}
        \begin{figure}[h]
        \centering
        \includegraphics[width=1\linewidth]{pic/sgd_stanford.png}
        \caption{\footnotesize SGD Variability (CS231n, Stanford)}
        \end{figure}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.5\linewidth}
        \begin{figure}
        \centering
        \includegraphics[width=0.5\linewidth]{pic/saddle_wiki.png}
        \caption{\footnotesize Saddle Point \url{(en.wikipedia.org)}}
        \end{figure}
    \end{minipage}
\end{frame}

\section{Momentum}
\begin{frame}{Problem Definition and Solution}
    \begin{minipage}{0.6\textwidth}
    \begin{itemize}
        \item \textbf{Objective}: Enhance the vanilla Gradient Descent algorithm to improve convergence and stability.
        \item \textbf{Challenges}:
        \begin{itemize}
            \item Selecting an appropriate learning rate is crucial to avoid slow convergence and getting stuck in local minima.
        \end{itemize}
        \item \textbf{Proposed Solution}:
        \begin{itemize}
            \item Instead of testing multiple learning rates, which can be inefficient and time-consuming, incorporate \textbf{Momentum} to adaptively adjust the learning rate based on oscillations:
            \begin{itemize}
                \item Increase steps in stable directions.
                \item Decrease steps in oscillating directions.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \end{minipage}%
    \begin{minipage}{0.3\textwidth}
        \begin{figure}[h]
        \centering
        \includegraphics[height=0.2\textheight]{pic/momentum_paperswithcode.jpg}
        \caption{\footnotesize Momentum smooths oscillations and accelerates progress. Source: paperswithcode}
        \end{figure}
    \end{minipage}
\end{frame}

\begin{frame}{Introduction to Momentum in Optimization}
    \begin{itemize}
        \item \textbf{Origin of Momentum}: 
        \begin{itemize}
            \item Inspired by Newtonian physics, momentum in optimization uses the concept of velocity in motion, accumulating gradient history to smooth the learning trajectory, akin to an object moving based on past inertia.
            \item Initially introduced to tackle challenges in gradient descent, where inconsistent gradients or noisy updates lead to erratic and slow convergence.
        \end{itemize}
        
        \item \textbf{Purpose of Momentum}: 
        \begin{itemize}
            \item \textbf{Dampens Oscillations}: Utilizes prior gradients to minimize oscillations along steep or erratic regions, resulting in a smoother and more stable path.
            \item \textbf{Speeds Up Convergence}: Particularly effective in narrow valleys or flat regions, where standard gradient descent may struggle or oscillate, causing slow progress.
        \end{itemize}
        
        \item \textbf{Mechanism}: 
        \begin{itemize}
            \item Achieves improvements by adding a "velocity" term to gradient descent, facilitating consistent directional movement and balancing update sizes for faster and more reliable convergence.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{How Momentum Accelerates Learning}
    \begin{itemize}
        \item In momentum-based optimization, updates are influenced by a weighted combination of the current gradient and previous updates.
        \item \textbf{Why Use Momentum?}
        \begin{itemize}
            \item \textbf{Reduces Oscillations}: Momentum softens oscillations along steeper directions while preserving progress in directions with smaller gradients, resulting in a balanced path.
            \item \textbf{Increases Efficiency}: By "accelerating" updates in directions of consistent gradient descent, momentum minimizes unnecessary steps and decreases overall convergence time.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{First Moment (Momentum)}
\begin{frame}{First Moment (Momentum)}
    \begin{itemize}
        \item \textbf{Definition}: The first moment, \( m_t \), represents a moving average of past gradients. It builds "velocity" that propels learning in a consistent direction.
        \item \textbf{Update Rule}:
        \[
        m_{t+1} = \beta_1 m_t + (1 - \beta_1) \nabla_w J(w_t)
        \]
        \[
        w_{t+1} = w_t - \eta m_{t+1}
        \]
        where:
        \begin{itemize}
            \item \( \beta_1 \): Decay rate, usually 0.9 or 0.99, which controls the weight of past gradients.
            \item \( \eta \): Learning rate.
        \end{itemize}
        \item \textbf{Why Use First Momentum?}
        \begin{itemize}
            \item Inspired by the idea of rolling momentum, it smooths and accelerates learning by sustaining direction from prior gradients.
            \item This type of momentum is ideal for traversing narrow valleys or regions where standard gradient descent would oscillate.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Second Moment (Variance)}
\begin{frame}{Second Moment (Variance)}
    \begin{itemize}
        \item \textbf{Definition}: The second moment, \( v_t \), represents the moving average of squared gradients. It measures the gradient magnitude over time.
        \item \textbf{Update Rule}:
        \[
        v_{t+1} = \beta_2 v_t + (1 - \beta_2) (\nabla_w J(w_t))^2
        \]
        \[
        w_{t+1} = w_t - \frac{\eta}{\sqrt{v_{t+1} + \epsilon}} \nabla_w J(w_t)
        \]
        where:
        \begin{itemize}
            \item \( \beta_2 \): Decay rate for variance (usually 0.99 or 0.999).
            \item \( \epsilon \): Small constant to prevent division by zero.
        \end{itemize}
        \item \textbf{Why Use Second Momentum?}
        \begin{itemize}
            \item Adjusts step size based on gradient magnitude, preventing large steps when gradients are large and accelerating learning when they are small.
            \item Useful in adaptive methods like Adam, where the combination of first and second moments allows stable and efficient convergence across various scenarios.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Moment Bias Correction}
    \begin{itemize}
        \item \textbf{Problem}: When we start training, both $m_{t}$ and $v_{t}$ are initialized to zero, causing their estimates to be biased toward zero in the early steps, especially when gradients are small.
        \item \textbf{Solution}: We use bias-corrected versions of $m_{t}$ and $v_{t}$ to address this:
        \[\hat{m}_{t} = \frac{m_{t}}{1 - \beta_1^{t}}, \quad 
        \hat{v}_{t} = \frac{v_{t}}{1 -\beta_2^{t}}\]
        \item These corrections compensate for the bias by scaling $m_{t}$ and $v_{t}$ upward, especially in the early steps when $t$ is small, ensuring more accurate estimates of the moments.
    \end{itemize}
\end{frame}

\subsection{Adam: Adaptive Moment Estimation}
\begin{frame}{Introduction to Adam Optimizer}
    \begin{itemize}
        \item \textbf{Origin and Purpose}: 
        \begin{itemize}
            \item Proposed in 2014 by Diederik Kingma and Jimmy Ba, Adam (Adaptive Moment Estimation) addresses key limitations in earlier optimization methods by combining aspects of \textbf{momentum} and \textbf{adaptive learning rates}.
            \item Adam is designed to handle sparse gradients and noisy updates by adjusting the learning rate for each parameter based on historical gradients.
        \end{itemize}
        \item \textbf{Core Idea}:
        \begin{itemize}
            \item Adam optimizes by maintaining two moving averages — the \textbf{first moment (mean of gradients)} and the \textbf{second moment (variance of gradients)} — allowing it to adapt learning rates for each parameter individually.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Adam’s Adaptive Learning Rate Mechanism}
    \begin{itemize}
        \item \textbf{Why Adaptive Rates?}
        \begin{itemize}
            \item Unlike traditional SGD, Adam adapts the learning rate for each parameter based on recent gradient magnitudes.
            \item Large gradients lead to reduced update sizes, while smaller gradients allow larger updates, balancing convergence speed and stability.
        \end{itemize}
        \item \textbf{Moment Tracking}
        \begin{itemize}
            \item The \textbf{first moment} ($m_t$) tracks the mean of gradients to provide momentum.
            \item The \textbf{second moment} ($v_t$) tracks squared gradients, enabling Adam to normalize updates and prevent sudden changes in direction.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Mathematical Formulation of Adam}
    \begin{itemize}
        \item \textbf{Adam Update Rules}:
        \begin{itemize}
            \item First moment estimate:
            \[
            m_{t+1} = \beta_1 m_t + (1 - \beta_1) \nabla_w J(w_t)
            \]
            \item Second moment estimate:
            \[
            v_{t+1} = \beta_2 v_t + (1 - \beta_2) (\nabla_w J(w_t))^2
            \]
            \item Bias-corrected moments to address initialization bias:
            \[
            \hat{m}_{t+1} = \frac{m_{t+1}}{1 - \beta_1^{t+1}}, \quad 
            \hat{v}_{t+1} = \frac{v_{t+1}}{1 - \beta_2^{t+1}}
            \]
            \item Update step for parameter \( w_t \):
            \[
            w_{t+1} = w_t - \eta \frac{\hat{m}_{t+1}}{\sqrt{\hat{v}_{t+1}} + \epsilon}
            \]
        \end{itemize}
        where \(\beta_1\), \(\beta_2\) are decay rates, and \(\epsilon\) is a small constant to prevent division by zero.
    \end{itemize}
\end{frame}

% \begin{frame}{Advantages and Convergence of Adam}
%     \begin{itemize}
%         \item \textbf{Benefits of Adam}:
%         \begin{itemize}
%             \item Combines the benefits of \textbf{momentum} (to reduce oscillations) and \textbf{RMSprop} (to adapt learning rates), making it particularly effective in noisy and sparse gradients.
%             \item \textbf{Fast Convergence}: Adam adapts learning rates per parameter, providing faster convergence in many cases.
%             \item \textbf{Reduced Hyperparameter Tuning}: By automatically adjusting update sizes, Adam requires less hyperparameter tuning than SGD.
%         \end{itemize}
%         \item \textbf{Convergence}:
%         \begin{itemize}
%             \item The original Adam paper guarantees convergence by using \textbf{decaying learning rates} and adjusting for biases in moment estimation.
%             \item Empirically shown to be effective across a wide range of problems and architectures.
%         \end{itemize}
%     \end{itemize}
% \end{frame}


% \begin{frame}{Comparison of Momentum Methods}
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \begin{center}
%         \begin{figure}
%         \centering
%             \includegraphics[width=1\linewidth]{pic/gd_comparison.png}
%             \caption{\footnotesize GD comparison from \url{https://github.com/ilguyi/optimizers.numpy}}
%         \end{figure}
%         \end{center}
%     \end{minipage}%
%     \begin{minipage}{0.5\textwidth}
%         \centering
%         \begin{center}
%         \begin{figure}
%         \centering
%             \includegraphics[width=0.9\linewidth]{pic/Adam_training_kingma2015.png}
%             \caption{\footnotesize GD comparison on MNIST from \url{kingma2014adam}}
%             %\cite{kingma2014adam}
%         \end{figure}
%         \end{center}
%     \end{minipage}
% \end{frame}

\section{References}
\begin{frame}[allowframebreaks]
    \bibliography{ref}
    %\bibliographystyle{ieeetr}
    \nocite{*} % used here because no citation happens in slides
    % if there are too many try use：
    \tiny\bibliographystyle{ieeetr}
\end{frame}
\end{document}