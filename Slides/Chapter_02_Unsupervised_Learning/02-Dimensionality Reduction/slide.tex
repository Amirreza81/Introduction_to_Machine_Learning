%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Beamer template for University of Wollongong     %
% Based on THU beamer theme                          %
% Author: Qiuyu Lu                                   %
% Date: July 2024                                    %
% LPPL Licensed.                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Customized for Sharif University of Technology     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[serif, aspectratio=169]{beamer}
%\documentclass[serif]{beamer}  % for 4:3 ratio
\usepackage[T1]{fontenc} 
\usepackage{fourier} % see "http://faq.ktug.org/wiki/uploads/MathFonts.pdf" for other options
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{lipsum}
\usepackage{algorithm, algpseudocode, mathtools, needspace}
\usepackage{tikz}



\author{Ali Sharifi-Zarchi}
\title{Machine Learning (CE 40717)}
\subtitle{Fall 2024}
\institute{
    CE Department \\
    Sharif University of Technology
}
%\date{\small \today}
% \usepackage{UoWstyle}
\usepackage{SUTstyle}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}


\begin{document}

\begin{frame}
    \titlepage
    \vspace*{-0.6cm}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/sharif-main-logo.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Introduction}

\begin{frame}{High Dimensional Data}
    \begin{itemize}
        \item High-Dimensions  = Lots of	Features
        \item EEG Signals of Brain 
        56 Channels $*$ 3000 Time Points For Each Trial

        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.4]{pic/brain.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{High Dimensional Data}
    \begin{itemize}
        \item High-Dimensions  = Lots of	Features
        \item Customer Purchase Data
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.4]{pic/customer_data.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{Dimensionality Reduction }
    \begin{itemize}
        \item \textbf{Feature Selection}
            \begin{itemize}
                \item Select a subset of a given feature set
            \end{itemize}

         \item \textbf{Feature Extraction}
            \begin{itemize}
                \item A linear or non-linear transform on the original feature space
            \end{itemize}
            \begin{figure}[htpb]
                \begin{center}
                    \includegraphics[keepaspectratio, scale=0.25]{pic/FS-FE.png}
                \end{center}
            \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}{Dimensionality Reduction}
    \begin{itemize}
        \item Maximize the retention of  \textbf{important information} while reducing the dimensionality
        
        \item What is information?
    \end{itemize}
\end{frame}

\begin{frame}{Dimensionality Reduction}
    \begin{itemize}
        \item Maximize the retention of  \textbf{important information} while reducing the dimensionality
        \item \textbf{Information:} Variance of whole data
        \begin{figure}[htpb]
                \begin{center}
                    \includegraphics[keepaspectratio, scale=0.5]{pic/dim_red_var.JPG}
                    \caption{\href{https://bookdown.org/tpinto_home/Unsupervised-learning/principal-components-analysis.html}{Figure reference}}
                \end{center}
        \end{figure}
    \end{itemize}

\end{frame}

\begin{frame}{Dimensionality Reduction}
    \begin{itemize}
        \item Maximize the retention of  \textbf{important information} while reducing the dimensionality
        \item \textbf{Information:} Local relationships
        \begin{figure}[htpb]
                \begin{center}
                    \includegraphics[keepaspectratio, scale=0.35]{pic/dim_red_local.JPG}
                  \caption{\href{https://datascience.stackexchange.com/questions/34146/what-does-the-long-curve-shape-t-sne-mean}{Figure reference}}
                \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}{Dimensionality Reduction}
    \begin{itemize}
        \item Maximize the retention of  \textbf{important information} while reducing the dimensionality
        \item \textbf{Information:} Statistical independence
        \begin{figure}[htpb]
                \begin{center}
                    \includegraphics[keepaspectratio, scale=0.25]{pic/dim_red_independent.JPG}
                \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}{Dimensionality Reduction Benefits}
    \begin{itemize}
        \item \textbf{Visualization}
            \begin{itemize}
                \item Project high dimensional data into 2D or 3D
            \end{itemize}
        \item \textbf{More efficient use of resources}
            \begin{itemize}
                \item Time, Memory, CPU
            \end{itemize}
        \item \textbf{Pre-process}
            \begin{itemize}
                \item Improve accuracy by reducing features
                \item As a Preprocessing step to reduce dimensions for supervised learning tasks
                \item Helps avoiding overfitting
            \end{itemize}
        \item \textbf{Removing Noise}
    \end{itemize}
\end{frame}

\section{Principal Component Analysis (PCA)}

\begin{frame}{Idea}
    \begin{itemize}
        \item Given data points in a d-dimensional space, project them into a lower dimensional space while preserving as much information as possible,
        \begin{itemize}
            \item Find best planar approximation of 3D data
            \item Find best 12-D approximation of 104-D data
        \end{itemize}
        \item In particular, choose projection that minimizes squared error in reconstructing the original data
    \end{itemize}
\end{frame}

\begin{frame}{Idea}
    \begin{itemize}
        \item 2D Gaussian dataset
         \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.6]{pic/pcaData.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}{Idea}
    \begin{itemize}
        \item 2D Gaussian dataset
        \item First PCA axis
         \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.6]{pic/pcaData1.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}{Idea}
    \begin{itemize}
        \item 2D Gaussian dataset
        \item First and second PCA axes
         \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.6]{pic/pcaData2.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}{Random vs Principal Projection}
    \begin{itemize}
        \item Random direction vs. principal component
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.8]{pic/pcaVSrandom.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{Definition}
    \begin{itemize}
        \item \textbf{Goal}: reducing the dimensionality of the data while preserving important aspects of the data
        \item Suppose $\mathbf{X} = \begin{pmatrix} \mathbf{X}_1^T \\ \vdots \\ \mathbf{X}_n^T \end{pmatrix}_{n \times d}$
        \item $\mathbf{X}_{n \times d} \xrightarrow{\text{PCA}} \tilde{\mathbf{X}}_{n \times k} \quad \text{with} \quad k \leq d$
        \item \textbf{assumption}: $\mu_x = \frac{1}{M} \sum_{i=1}^{M}X_i = 0_{d \times 1}$
    \end{itemize}
\end{frame}

\begin{frame}{Interpretations}
    Orthogonal projection of the data onto a \textbf{lower-dimensional} linear space that:
    \begin{itemize}
        \item \textcolor{gray}{Interpretation 1.} Maximizes variance of projected data
        \item \textcolor{gray}{Interpretation 2.} Minimizes the sum of squared distances to the line
    \end{itemize}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.4]{pic/pca.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}{Equivalence of the interpretations}
    \begin{itemize}
        \item Minimizing the sum of square distances to the line is \textbf{equivalent} to maximizing the sum of squares of the projections on that line.
    \end{itemize}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.55]{pic/var_vs_rec.JPG}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}{Equivalence of the interpretations}

    \textbf{Principal Components (PCs)}: A set of \textbf{orthonormal} vectors $(\nu = [\nu_1, \nu_2, ...,\nu_k])$ generated by PCA, which fulfill both of the \textcolor{gray}{interpretations}.\\[0.4cm]
    
    \textcolor{gray}{Interpretation 1.} Maximizes variance of projected data\\[0.3cm]
    \begin{itemize}
        \item Projection of data points on $\nu_1$
        $$\Pi = \Pi_{\nu_1} \{ X_{1}, \dots, X_{n} \} = \{ \nu_1^T X_{1}, \dots, \nu_1^T X_{n} \}$$
        \item Note that $\text{Var}(X) = \mathbb{E}[X^2] - \mathbb{E}[X]^2$
        $$\text{Var}(\Pi) = \frac{1}{N} \left( \sum_{i=1}^{N} (\nu_1^T X_i)^2 \right) - \left( \frac{1}{N} \sum_{i=1}^{N} \nu_1^T X_i \right)^2$$
    \end{itemize}
    
\end{frame}

\begin{frame}{Equivalence of the interpretations}
    \textcolor{gray}{Interpretation 1.} Maximizes variance of projected data\\[0.3cm]
    \begin{itemize}
        \item Based on the assumption, $\frac{1}{N} \sum_{i=1}^{N} X_i = 0$
        $$\nu^T(\frac{1}{N} \sum_{i=1}^{N} X_i) = 0 \quad \longrightarrow \quad \frac{1}{N} \sum_{i=1}^{N} \nu_1^T X_i = 0$$
        \item So,
        $$\text{Var}(\Pi) = \frac{1}{N} \sum_{i=1}^{N} (\nu_1^T X_i)^2$$
        \item To find $\nu_1$ that maximizes the variance
        $$\begin{aligned}
            \argmax_{\nu_1} \quad & \frac{1}{N} \sum_{i=1}^{N} (\nu_1^T X_i)^2\\
            \textrm{s.t.} \quad & \nu_1^T\nu_1 = 1\\
        \end{aligned}$$
    \end{itemize}
\end{frame}

\begin{frame}{Equivalence of the interpretations}
    \textcolor{gray}{Interpretation 2.} Minimizes the sum of squared distances to the line\\[0.3cm]
    \minipage{0.54\textwidth}
    \begin{itemize}
        \item Squared distance of one point to the line $$\left\| X_i - (\nu_1^{T}X_i) \nu_1 \right\|_2^2 \quad \text{which } \nu_1^TX_i \text{ is scalar.}$$
        \item Sum of squared distances $$L_1 = \frac{1}{N} \sum_{i=1}^{N} \left\| X_i - (\nu_1^{T}X_i) \nu_1 \right\|_2^2$$
    \end{itemize}
    \endminipage
    \hfill
    \minipage{0.42\linewidth}
    \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.7]{pic/varpic.jpg}
            \end{center}
        \end{figure}
    \endminipage
    \vspace{0.1cm}
    \begin{itemize}
        \item By Pythagorean theorem $$L_1 = \frac{1}{N} \sum_{i=1}^{N} (\left\| X_i \right\|^2 - (\nu_1^{T}X_i)^2 \left\|\nu_1 \right\|_2^2) = \underbrace{\frac{1}{N} \sum_{i=1}^{N} (\left\| X_i \right\|^2)}_{\text{constant}} - \frac{1}{N} \sum_{i=1}^{N} ((\nu_1^{T}X_i)^2 \left\|\nu_1 \right\|_2^2) $$
    \end{itemize}
\end{frame}

\begin{frame}{Equivalence of the interpretations}
    \textcolor{gray}{Interpretation 2.} Minimizes the sum of squared distances to the line\\[0.2cm]
    \begin{itemize}
        \item Removing constant to minimize
        \item Based on orthonormality, $||V_1||_2^2 = 1$
        \item To find $\nu_1$ that minimizes the sum of squared distances
    \end{itemize}
    \vspace{0.2cm}
    \minipage{0.45\textwidth}
    $$\begin{aligned}
            \argmin_{\nu_1} \quad & -\frac{1}{N} \sum_{i=1}^{N} (\nu_1^T X_i)^2\\
            \textrm{s.t.} \quad & \nu_1^T\nu_1 = 1\\
        \end{aligned}$$
    \endminipage
    \hfill
    \minipage{0.1\textwidth}
    $$\equiv$$
    \endminipage
    \hfill
    \minipage{0.45\linewidth}
    $$\begin{aligned}
            \argmax_{\nu_1} \quad & \frac{1}{N} \sum_{i=1}^{N} (\nu_1^T X_i)^2\\
            \textrm{s.t.} \quad & \nu_1^T\nu_1 = 1\\
        \end{aligned}$$
    \endminipage
    \vspace{0.5cm}
    \begin{itemize}
        \item So, the two \textcolor{gray}{interpretations} are equivalent.
    \end{itemize} 
\end{frame}

\begin{frame}{Pre-processing}
    \begin{itemize}
        \item \textbf{Center the data}
            \begin{itemize}
                \item \textbf{Zero}ing out the \textbf{mean} of each feature\\
            \end{itemize}
            
        \item \textbf{Scaling to normalize each feature to have variance 1 (An arbitrary step)}
            \begin{itemize}
                \item The final result may be wrong!
                \item It helps when unit of measurements of features are different and some features may be ignored without normalization
            \end{itemize}
    \end{itemize}
\end{frame}


\begin{frame}{Pre-processing}
    \begin{itemize}
        \item Scaling to normalize each feature may affect the final result!!
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.6]{pic/pcaScaling.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{Algorithms}
    \begin{itemize}
        \item Algorithm 1: sequential
        \item Algorithm 2: sample covariance matrix
    \end{itemize}
\end{frame}

\subsection{Sequential Algorithm}

\begin{frame}{Sequential Algorithm}
    \begin{itemize}
        \item First view
        \begin{itemize}
            \item Find directions with the maximum variations 
        \end{itemize}
        $$
        \max_{v_1} \frac{1}{N} \sum_{n=1}^{N} (v_1^T x_n)^2 = \frac{1}{N} \sum_{n=1}^{N} v_1^T (x_n x_n^T) v_1 = v_1^T \left( \frac{1}{N} \sum_{n=1}^{N}(x_n x_n^T) \right) v_1 = v_1^T S {v_1}
        $$
        $$ \text{s.t. } v_1^{T}{v_1}  = 1$$
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.6]{pic/var_vs_rec.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

% \begin{frame}{Sequential Algorithm}
%     \begin{itemize}
%         \item Find directions with the maximum variations
%         $$
%         \argmax_{v_1} \frac{1}{N} \sum_{n=1}^{N} (v_1^T x_n)^2 = \frac{1}{N} \sum_{n=1}^{N} v_1^T (x_n x_n^T) v_1 = v_1^T \left( \frac{1}{N} \sum_{n=1}^{N}(x_n x_n^T) \right) v_1 = v_1^T S {v_1}
%         $$
%         $$ \text{s.t. } v_1^{T}{v_1}  = 1$$
%         \item   Using Lagrangian multiplier technique
%         $$ L(v_1, \lambda) = v_1^T S v_1 + \lambda (1 - v_1^T v_1) $$
%         $$ \frac{\partial L}{\partial v_1} = 0 \Rightarrow 2Sv_1 - 2\lambda v_1 = 0$$
%         $$ \Rightarrow Sv_1 = \lambda v_1 $$
%     \end{itemize}
% \end{frame}

\begin{frame}{Sequential Algorithm}
    \begin{itemize}
        \item To find $v_2$, we maximize the variance of the projection in the residual subspace
$$
v_2 = \argmax_{v_2} \left( \frac{1}{N} \sum_{i=1}^{N} \left( x_i - v_1^T x_i \right)^2 \right)
$$
$$ \text{s.t. } v_2^{T}{v_2}  = 1$$
          \item To find $v_k$, we maximize the variance of the projection in the residual subspace
$$
v_k = \argmax_{v_k} \left( \frac{1}{N} \sum_{i=1}^{N} \left( x_i - \sum_{j=1}^{k-1} W_j^T x_i \right)^2 \right)
$$      
$$\text{s.t. } v_k^{T}{v_k}  = 1$$
    \end{itemize}
\end{frame}

% \begin{frame}{Sequential Algorithm}
%     \begin{itemize}
%         \item Second view
%         \begin{itemize}
%             \item Find directions with the minimum reconstruction error 
%         \end{itemize}
%         $$
%         \min_{v1} \sum_{n=1}^{N} \lVert x_n - (v_{1}^{T} x_n)v_{1} \rVert_2^{2}
%         $$
%         $$ \text{s.t. } v_1^{T}{v_1}  = 1$$
%         \item Show this has an equal solution with the first view

%         \begin{figure}[htpb]
%             \begin{center}
%                 \includegraphics[keepaspectratio, scale=0.55]{pic/var_vs_rec.JPG}
%             \end{center}
%         \end{figure}

%     \end{itemize}
% \end{frame}


\begin{frame}{Sequential Algorithm}
    \begin{itemize}
       \item As we have $S v_j = \lambda_j v_j$,

    $$\Rightarrow \text{var}(v_j^T x) = v_j^T x x^{T} v_j =
       v_j^T S v_j =
       \lambda_j v_j^T v_j =
       \lambda_j$$.
    
    \item The variance along an eigenvector  $v_j \text{ equals the eigenvalue } \lambda_j. $
        
    \end{itemize}
\end{frame}

\begin{frame}{Sequential Algorithm}
    \begin{itemize}
       \item Eigenvalues: $\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq ... $
        \begin{itemize}
            \item The first PC $v_1$ is the the eigenvector of the sample covariance matrix $S$ associated with the largest eigenvalue
            \item The 2nd PC $v_2$ is the the eigenvector of the sample covariance
matrix $S$ associated with the second largest eigenvalue
            \item And so on ...
        \end{itemize}
         \item To reduce the dimension of the data to k, we select eigenvectors with the top k eigenvalues
        
    \end{itemize}
\end{frame}


\subsection{Sample Covariance Matrix Algorithm}

\begin{frame}{Sample Covariance Matrix}
    \begin{itemize}
        \item Given data ${x_1, …, x_n}$, compute covariance matrix $\Sigma$
        $$\Sigma = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})(x_i - \bar{x})^T \text{ where }  \bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i $$
        \item PCA basis vectors = the eigenvectors of $\Sigma$
        \item Larger eigenvalue $\rightarrow$ more important eigenvectors
    \end{itemize}
\end{frame}

% \begin{frame}{}
%     \begin{itemize}
%         \item Input: $N * d$ data matrix $X$ (each row contain a $d$ dimensional data point)
%         \begin{itemize}
%             \item $\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i$
%             \item $\tilde{X} \leftarrow $ Mean value of data points is subtracted from rows of $X$
%             \item $S = \frac{1}{N}\tilde{X}^{T}\tilde{X} \text{(Covariance matrix)}$
%             \item Calculate eigenvalue and eigenvectors of $S$
%             \item Pick $K$ eigenvectors corresponding to the largest eigenvalues
% and put them in the columns of $A = [v_1, ... , v_k]$
%             \item $X' = XA$
%         \end{itemize}
%     \end{itemize}
% \end{frame}

\begin{frame}{Sample Covariance Matrix}
    Sample covariance matrix 
    \begin{itemize}
        \item It is symmetric $\Rightarrow$ Eigen-vectors are \textbf{orthogonal}
        \item It is symmetric $\Rightarrow$ Eigen-values are \textbf{real}
        \item It is positive semidefinite $\Rightarrow$ Eigen-values are \textbf{non-negative}
    \end{itemize}
\end{frame}

\begin{frame}{Sample Covariance Matrix}
    Principal component analysis
    \begin{itemize}
        \item Principal components are \textbf{orthonormal} 
        \item Variances along each principal component are \textbf{real}
        \item Variances along each principal component are \textbf{non-negative}
    \end{itemize}
\end{frame}

\begin{frame}{Sample Covariance Matrix}
    Principal component analysis and sample covariance matrix
    \begin{itemize}
        \item Principal components are \textbf{eigen-vectors} 
        \item Variance of each principal component is the \textbf{eigen-value} of the corresponding eigen-vector
    \end{itemize}
\end{frame}

\begin{frame}{Sample Covariance Matrix}
    \begin{algorithm}[H]
    \caption{Sample Covariance Matrix}\label{alg:Sample Covariance Matrix}
    \begin{algorithmic}[1]
         \State \textbf{Input:} $X \in \mathbb{R}^{N \times d}$ (data matrix with $N$ data points and $d$ dimensions)
        \State Compute the mean of each feature: 
        \LeftComment{$\bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i$}
        \State Subtract the mean from each data point (center the data): 
        \LeftComment{$\tilde{X} \leftarrow X - 1_N\bar{x}^{T}$}
        \State Compute the covariance matrix: 
        \LeftComment{$S = \frac{1}{N} \tilde{X}^{T} \tilde{X}$}

        \State Compute the eigenvalues and eigenvectors of $S$: 
        \LeftComment{$[\lambda_1, \lambda_2, \dots, \lambda_d],  [v_1, v_2, \dots, v_d] = \text{eig}(S)$}
        \State Select the top $K$ eigenvectors corresponding to the largest eigenvalues: 
        \LeftComment{$A \leftarrow [v_1, v_2, \dots, v_K]$}
        \State Transform the data into the new subspace:
        \LeftComment{$X' \leftarrow X \cdot A$}
        \State \textbf{Output:} $X' \in \mathbb{R}^{N \times K}$ (transformed data with reduced dimensions)
    \end{algorithmic}
    \end{algorithm}
\end{frame}

\section{Choose PCs}

\begin{frame}{How many PCs?}
    \begin{itemize}
        \item For $n$ original dimensions, sample covariance matrix is $n * n$, and has up to $n$ eigenvectors. So $n$ PCs
        \item Can ignore the components of lesser significance
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.6]{pic/var.JPG}
            \end{center}
        \end{figure}
        \item You do lose some information, but if the eigenvalues are small, you don’t lose much
    \end{itemize}
\end{frame}

\begin{frame}{How many PCs?}
    \begin{minipage}{0.4\textwidth}
         \begin{itemize}
            \item Select the desired variance ratio and select the PCs 
        \end{itemize}
         $$\min_k \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{d} \lambda_i} >= 0.9$$
    \end{minipage}
    \begin{minipage}{0.55\textwidth}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[keepaspectratio, scale=0.4]{pic/PCA_var.JPG}
            \end{center}
        \end{figure}
    \end{minipage}
\end{frame}

\section{Applications}

\begin{frame}{Image Compression}
       \begin{itemize}
           \item Divide the original 372x492 image into patches
\begin{itemize}
    \item Each patch is an instance that contains 12x12 pixels on a grid
\end{itemize}
            \item Consider each as a 144-D vector
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/original.JPG}
            \end{center}
        \end{figure}
\end{frame}

\begin{frame}{Image Compression}
       \begin{itemize}
           \item 144D $\Rightarrow$ 60D
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/60d.JPG}
            \end{center}
        \end{figure}
\end{frame}

\begin{frame}{Image Compression}
       \begin{itemize}
           \item 144D $\Rightarrow$ 16D
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/16d.JPG}
            \end{center}
        \end{figure}
\end{frame}


\begin{frame}{Image Compression}
       \begin{itemize}
           \item 16 most important eigenvectors
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/16_most.JPG}
            \end{center}
        \end{figure}
\end{frame}


\begin{frame}{Image Compression}
       \begin{itemize}
           \item 144D $\Rightarrow$ 3D
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/3d.JPG}
            \end{center}
        \end{figure}
\end{frame}


\begin{frame}{Image Compression}
       \begin{itemize}
           \item 3 most important eigenvectors
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/3_most.JPG}
            \end{center}
        \end{figure}
\end{frame}


\begin{frame}{Image Compression}
       \begin{itemize}
           \item L2 error and PCA dim
       \end{itemize}
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/error.JPG}
            \end{center}
        \end{figure}
\end{frame}

\section{Shortcomings}

\begin{frame}{Class Labels}
    \begin{itemize}
        \item PCA doesn’t know about class labels!
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/labels.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{Non-Linear}
    \begin{itemize}
        \item PCA cannot capture Non-Linear structure!
        \begin{figure}[htpb]
            \begin{center}
                \includegraphics[width=8cm, height=5cm]{pic/nolinear.JPG}
            \end{center}
        \end{figure}
    \end{itemize}
\end{frame}

\section{Conclusion}

\begin{frame}{Conclusion}
    \begin{itemize}
        \item PCA 
        \begin{itemize}
            \item finds orthonormal basis for data
            \item Sorts dimensions in order of “importance”
            \item Discard low significance dimensions
        \end{itemize}
        \item Applications
        \begin{itemize}
            \item Get compact description
            \item Remove noise
            \item Improve classification (hopefully)
            \item More efficient use of resources
            \item Statistical
        \end{itemize}
        \item Not magic
        \begin{itemize}
            \item  Doesn’t know class labels
            \item Can only capture linear variations
        \end{itemize}
        \item One of many tricks to reduce dimensionality!
    \end{itemize}
\end{frame}

\section{References}

\begin{frame}{References}
    \begin{itemize}
        \item Advanced Introduction to Machine Learning, CMU-10715, Barnabás Póczos
        \item 10-701 Introduction to Machine Learning, CMU, Matt Gormley	
        \item 10-301/10-601 Introduction to Machine Learning, CMU, Matt	Gormley	
        \item CE-477: Machine Learning - CS-828: Theory of Machine Learning, Sharif University of Technology, Fatemeh Seyyedsalehi
    \end{itemize}
\end{frame}

\end{document}