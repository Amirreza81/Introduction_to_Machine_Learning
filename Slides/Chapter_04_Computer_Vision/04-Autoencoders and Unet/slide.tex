%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Beamer template for University of Wollongong     %
% Based on THU beamer theme                          %
% Author: Qiuyu Lu                                   %
% Date: July 2024                                    %
% LPPL Licensed.                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Customized for Sharif University of Technology     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[serif, aspectratio=169]{beamer}
%\documentclass[serif]{beamer}  % for 4:3 ratio
\usepackage[T1]{fontenc} 
\usepackage{fourier} % see "http://faq.ktug.org/wiki/uploads/MathFonts.pdf" for other options
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{lipsum}
\usepackage{array}
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsfonts}

\usetikzlibrary{positioning,calc}





\author{Ali Sharifi-Zarchi}
\title{Machine Learning (CE 40717)}
\subtitle{Fall 2024}
\institute{
    CE Department \\
    Sharif University of Technology
}
%\date{\small \today}
% \usepackage{UoWstyle}
\usepackage{SUTstyle}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}


% Define custom colors
\definecolor{blue!20}{RGB}{173, 216, 230}
\definecolor{green!20}{RGB}{144, 238, 144}
\definecolor{red!20}{RGB}{255, 182, 193}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}


\begin{document}

\begin{frame}
    \titlepage
    \vspace*{-0.6cm}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/sharif-main-logo.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}

\section{Latent Variable Models}


\begin{frame}{What Is Latent Variable?}
    \begin{itemize}
        \item \textbf{Definition:} Latent variables are variables that are not directly observed but are inferred from other variables that are observed (measured).
    \end{itemize}
    
    \vspace{0.5cm} % Adjust the space as needed
    
    \begin{itemize}
        \item \textbf{Origin:} Derived from the Latin word \textit{latēre} meaning "hidden".
    \end{itemize}

    \vspace{0.5cm} % Adjust the space as needed
    
    \begin{itemize}
        \item \textbf{Examples:}
        \begin{itemize}
            \item Personality traits (in psychology)
            \item Economic factors like inflation (in economics)
            \item Topic of a document (in natural language processing)
        \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{What Is Latent Variable?}
      \begin{center}
        \includegraphics[width=0.5\textwidth]{pic/what is latent variable.png} 
    \end{center}
\end{frame}


\begin{frame}{What Does This Have To Do With Our Lesson?}
    \vspace{1cm}
    \begin{center}
        \textbf{How can we combine \textcolor{blue}{neural networks} and \textcolor{blue}{unlabeled data} to perform compression?}
    \end{center}
\end{frame}


\begin{frame}{Autoencoder: Background}
    \begin{center}
        \textbf{Using latent variables as a foundation}, an unsupervised approach for learning a lower-dimensional feature representation from unlabeled training data.
        
        \vspace{0.3cm}
        
        \begin{minipage}{0.6\textwidth}
            \centering
            \includegraphics[width=0.7\textwidth]{pic/AE background1.png}
        \end{minipage}
        \hspace{0.05\textwidth} 
        \begin{minipage}{0.3\textwidth}
            \raggedright
            Why do we care about a \\ low-dimensional \textit{$z$}?
        \end{minipage}
        
        \vspace{0.3cm}
        
        \textit{"Encoder"} learns mapping from the data, \textit{$x$}, to a low-dimensional latent space, \textit{$z$}
    \end{center}
\end{frame}

\begin{frame}{Autoencoders: Background}

    How can we learn this latent space? \\
    Train the model to use these features to \textbf{reconstruct the desired (usually original) data}.


    \begin{center}
                
        \includegraphics[width=0.7\textwidth]{pic/AE background2.png} 

        
        \textit{"Decoder"} learns mapping back from latent \textit{z}, to a reconstructed observation, \textit{x}
    \end{center}
\end{frame}

\begin{frame}{Autoencoders}
    \begin{enumerate}
        \item These models map between \textbf{\textcolor{deepblue}{observation space $\mathbf{x} \in \mathbb{R}^D$}} and \textbf{\textcolor{deepblue}{latent space $\mathbf{z} \in \mathbb{R}^Q$}}:
        
        \begin{itemize}
            \item \textbf{\textcolor{deepblue}{Encoder}} \quad \( f_w  :  \mathbf{x} \mapsto \mathbf{z} \)   \hspace{em} {\tiny{The "Analysis" net which computes the hidden representation}}
            \item \textbf{\textcolor{deepblue}{Decoder}} \quad \( g_w : \mathbf{z} \mapsto \mathbf{x} \)   \hspace{em} {\tiny{The "Synthesis" which recomposes the data from the hidden representation}}
        \end{itemize}
        
        \vspace{0.3cm}

        \item Each latent variable gets associated with each data point in the training set.
        \item The latent vectors are smaller than the observations \((Q < D) \Rightarrow\) compression.
        \item Models are linear or non-linear, deterministic or stochastic, with/without encoder.
    \end{enumerate}
    
    
    \begin{center}
\        \includegraphics[width=0.6\textwidth]{pic/AE1.png}\
    \end{center}
\end{frame}

\begin{frame}{Autoencoder Without a Non-linearity}
    
    \begin{center}
        \includegraphics[width=0.85\textwidth]{pic/AE without non-linearity.png}
    \end{center}
    
    
\begin{center}
    \fbox{\textbf{\textcolor{deepblue}{\(\mathbf{Y = WX} \)}}} \hspace{0.5cm} 
    \fbox{\textbf{\textcolor{deepblue}{\(\hat{\mathbf{X}} = W^{T} \mathbf{Y}\)}}} \hspace{0.5cm}
    \textbf{\textcolor{deepred}{\( E = \| \mathbf{X} - W^{T} W \mathbf{X} \|^2 \)}} \hspace{0.5cm}
    \footnotesize{Find \(W\) to minimize \(\text{Avg}[E]\)}
\end{center}

        
    \begin{itemize}
        \item \textbf{This is similar to PCA}
        \item[] \textendash \ The output of the hidden layer will be in the principal subspace
        \begin{itemize}
            \item[] \textbullet \ Even if the recomposition weights are different from the "analysis" weights
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{``Deep'' AE}
    \begin{itemize}
        \item \textbf{Deep nonlinear autoencoders} learn to project the data, not onto a subspace, but onto a nonlinear \textcolor{deepgreen}{manifold}.
        \vspace{0.1cm}
        
        \item This manifold is the \textbf{image of the decoder}.
        \vspace{0.1cm}
        
        \item This is a kind of \textcolor{deepgreen}{nonlinear dimensionality reduction}.
    \end{itemize}
    
    
    \begin{center}
        \includegraphics[width=\textwidth]{pic/Deep AE1.png} 
    \end{center}
    
\end{frame}

\begin{frame}{AE vs PCA}
    As we said, \textbf{PCA} can be described as
    \begin{center}
        \begin{minipage}{0.6\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{pic/AE vs PCA 1.png}
            \end{center}
        \end{minipage}
        \hfill
        \begin{minipage}{0.35\textwidth}
            \begin{align*}
                &\min_{W} \sum \| \hat{x} - x \|^2 \\
                &W^T W = I \\
                &\min_{W} \sum \| W^T W x - x \|^2
            \end{align*}
        \end{minipage}
        
        \vspace{0.3cm}
    \end{center}
    And also \textbf{Autoencoders} can be thought of as a \textbf{non-linear PCA}.
    \begin{center}                
        \begin{minipage}{0.6\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{pic/AE vs PCA 1.png} 
            \end{center}
        \end{minipage}
        \hfill
        \begin{minipage}{0.35\textwidth}
            \begin{align*}
                &\min_{h, g} \sum \| \hat{x} - x \|^2 \\
                &\min_{h, g} \sum \| g(f(x)) - x \|^2
            \end{align*}
        \end{minipage}
    \end{center}
\end{frame}

\begin{frame}{AE vs PCA}

    Nonlinear autoencoders can learn more powerful codes for a given dimensionality, compared with linear autoencoders (PCA).
    
    
    % Centered image
    \begin{center}
        \includegraphics[width=\textwidth]{pic/AE vs PCA 2.png} 
    \end{center}
\end{frame}


\begin{frame}{Autoencoder Key Characteristics}
\small
     
    \begin{itemize}
        \item \textbf{Specialized for Specific Data Type}
        \begin{itemize}
            \item Autoencoders excel at recognizing patterns in the type of data they were trained on, but may fail on different data types.
            \item Example: A model trained on animal images will not perform well when applied to landscapes.
        \end{itemize}
        
        \vspace{0.25cm}
        
        \item \textbf{Tailored Compression vs. General Algorithms}
        \begin{itemize}
            \item Unlike generic compression techniques (MP3, JPEG), which apply universal rules, autoencoders customize their approach to fit the training data structure.
        \end{itemize}
        
        \vspace{0.25cm}
        
        \item \textbf{Loss of Detail in Output}
        \begin{itemize}
            \item Autoencoders don’t retain all the original information, leading to some degradation in the reconstructed data.
            \item This makes them "lossy" – similar to formats like MP3 or JPEG, which prioritize size over perfect fidelity.
        \end{itemize}
    \end{itemize}
    

\end{frame}

\begin{frame}{Autoencoder Training }
    % Title and introductory text
    Encoder \textcolor{deepblue}{\textit{f}} and decoder \textcolor{deepblue}{\textit{g}}.    
    % Equation and diagram side by side
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{align*}
                f &: X \mapsto z \\
                g &: z \mapsto X \\
                & \arg\min_{f, g} \| x - (f \circ g)(x) \|^2
            \end{align*}
        \end{column}
        \begin{column}{0.7\textwidth}
            \includegraphics[width=0.5\textwidth]{pic/AE training1 .png}
        \end{column}
    \end{columns}

    \vspace{0.5cm}
    
    % Explanatory text below
    \textbf{An autoencoder is a feed-forward non-recurrent neural network.}
    
    \begin{itemize}
        \item With an input layer, an output layer, and one or more hidden layers
        \item It can be trained using the following technique:
            \begin{itemize}
                \item Compute gradients using back-propagation, followed by mini-batch gradient descent
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Fully Convolutional AE}
    \begin{center}
        \includegraphics[width=\textwidth]{pic/Fully Convolutional AE 1.png} 
    \end{center}
    
    \vspace{0.5cm}
    
    \begin{center}
        We will discuss more about \textbf{"Downsampling"} and \textbf{"Upsampling"} in the next section
    \end{center}
    
    \vspace{1cm}
    
    % Image credit
    \tiny{Image from \href{https://mriquestions.com/upsampling.html}{this link}}
\end{frame}

\begin{frame}{Autoencoders As Pretrained Models}
    \scriptsize
    \begin{itemize}
        \item \textbf{Feature Extraction:} Encoder \textbf{learns} data patterns and \textbf{compresses} them into useful features.
        \item \textbf{Supervised Model Initialization:} Use the \textbf{pretrained encoder} to initialize a supervised learning model.
        \item \textbf{Fine-Tuning:} Train the encoder and classifier \textbf{together} to improve performance.
        \item \textbf{Useful with Small Data:} Effective when training on \textbf{small datasets} by leveraging pretrained features.
    \end{itemize}
    
    \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/AEs as pretrained models.png} 
    \end{center}
\end{frame}

\begin{frame}{AE Performance \& Bottleneck Size (MNIST Example)}
    \begin{itemize}
        \item \textbf{Reconstruction Error and Bottleneck Size:}
        \begin{itemize}
            \item A good autoencoder should minimize reconstruction error.
            \item The error depends heavily on the size of the \( z \) (the dimension of the latent space).
        \end{itemize}
        
        \item \textbf{Experiment Setup:}
        \begin{itemize}
            \item CNN-based AE trained on MNIST with different bottleneck sizes:
            \begin{itemize}
                \item \((2, 1, 1)\), \((4, 1, 1)\) \& \((32, 1, 1)\)
            \end{itemize} 
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{AE Performance \& Bottleneck Size (MNIST Example) - Results}
    \begin{columns}[t] 
        \begin{column}{0.65\textwidth} 
            \begin{itemize}
                \item \textbf{Results:}
                \begin{itemize}
                    \item With \( d=2 \): Reconstruction is still good; a classifier could identify digits decently.
                    \item With \( d=32 \): Reconstruction is nearly perfect; despite a compression ratio of \(\sim0.04\) (from 32 to 28x28).
                \end{itemize}
                
                \item \textbf{Larger latent spaces} generally lead to \textbf{better reconstructions}, as more variables allow more information to be preserved during compression.
            \end{itemize}
        \end{column}
        
        % Right column with image
        \begin{column}{0.33\textwidth} % Adjust the width for better fit
            \begin{center}
                \includegraphics[width=0.9\textwidth, height=\textheight, keepaspectratio]{pic/AE Performance & Bottleneck Size (MNIST Example).png} 
                    \vspace{2.5cm}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Can The Decoder Of AE Be Used For Data Generation?}
    \small
    \begin{itemize}
        \item Autoencoders are effective data compressors but up to now do not yet show \textbf{data generation capabilities}.
        \item We already trained an AE on MNIST; then we use \textbf{random vectors} as input to the Decoder to generate new data.
        \begin{itemize}
            \item Is the \textbf{size} of the bottleneck affecting the \textbf{quality} of the generated sample?
        \end{itemize}
    \end{itemize}
    
    
    % Unified image at the bottom
    \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/AE for data generation1.png}
    \end{center}
\end{frame}


\begin{frame}{Can The Decoder Of AE Be Used For Data Generation?}
    Is \textbf{\textcolor{deepred}{any random vector}} we feed into decoder networks results in a valid output?
    \vspace{0.5cm}
    
    % Unified image at the bottom
    \begin{center}
        \includegraphics[width=\textwidth]{pic/AE for data generation2.png}
    \end{center}
\end{frame}


\begin{frame}{Can The Decoder Of AE Be Used For Data Generation?}
    \scriptsize
    \begin{itemize}
        \item \textbf{Varying the Latent Representation (z):} By changing the values in the latent space (z), the decoder can generate outputs that move along a \textbf{\textcolor{deepblue}{learned non-linear manifold}} (a curved space that represents the structure of the data).
        
        \item \textbf{Manifold Constraint:} The decoder can only generate data that lies on \textbf{\textcolor{deepblue}{the manifold learned from the training data}}. This means it cannot generate data outside of the patterns it was trained on.
        
        \item \textbf{Data Generation:} The decoder is an effective generator of data that follows the \textbf{\textcolor{deepblue}{distribution of the training set}}. Any value you feed into the latent space will result in an output that resembles the training data.
    \end{itemize}
    
    \vspace{0.5cm}
    
    % Unified image at the bottom
    \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/AE for data generation3.png}
    \end{center}
\end{frame}


\begin{frame}{Latent Space Properties}
    \begin{itemize}
        \item For the generation purpose, we need the latent space to show the following properties:
        \begin{itemize}
            \item \textbf{Continuity:} Similar points in the latent space will be similar after decoding.
            \item \textbf{Completeness:} Samples of the latent space lead to meaningful content after decoding.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Latent Space Properties: Continuity}
      \begin{center}
        \includegraphics[width=\textwidth]{pic/Latent space property-Continuity.png} 
    \end{center}
\end{frame}


\begin{frame}{Latent Space Properties: Completeness}
      \begin{center}
        \includegraphics[width=\textwidth]{pic/Latent space property-Completeness.png} 
    \end{center}
\end{frame}

\begin{frame}{AE Latent Space - MNIST}
    \begin{center}
        % Unified image
        \includegraphics[width=0.9\textwidth]{pic/AE Latent space - MNIST.png}
        {\color{deepred}\textbf{\\ What is the problem?}}
    \end{center}
\end{frame}

\begin{frame}{Latent Variable}
    \begin{itemize}
        \item We define a simple distribution on the latent space, i.e., \( p(z) \) and a decoder network \( p_\theta(\mathbf{x} | \mathbf{z}) \) to map the latent variable to the data space.
    \end{itemize}

    \vspace{0.1cm}

    \begin{equation*}
        p(\mathbf{x}) = \int p(\mathbf{x} | \mathbf{z}) p(\mathbf{z}) \, d\mathbf{z}
    \end{equation*}

    \begin{center}
        \includegraphics[width=0.8\textwidth]{pic/latent variable.png}
    \end{center}
\end{frame}


\begin{frame}{AE’s: Variational Autoencoder (Intuition)}
    \begin{center}
        \small The key change that they added was that instead of having an encoder that maps points in X to points in Z, VAE’s map points in X to distributions in Z. This allows a point in X to be indirectly mapped to several close neighbour points in Z.
    \end{center}


    \vspace{1.0cm}

    \scriptsize

    
        \begin{equation*}
        \mathbf{Simple\ AE} \ \Biggl\{\text{\ \begin{array}{ c }
        \text{input}\\
        \mathit{x}
        \end{array} \ } \ \ \xrightarrow{\text{\textcolor[rgb]{0.82,0.01,0.11}{Encoding}}\textcolor[rgb]{0.82,0.01,0.11}{\ }} \ \ \text{\ \begin{array}{ c }
        \text{latent\ representation}\\
        \mathit{\ z=e( x)}
        \end{array} \ } \ \ \ \xrightarrow{\text{\textcolor[rgb]{0.29,0.56,0.89}{Decoding}}} \ \text{\ \ \ \begin{array}{ c }
        \text{input\ reconstruction} \ \\
        \mathit{d( z)}
        \end{array}}\Biggr\}
        \end{equation*}
    
    \vspace{0.1cm}
    \hrule
    \vspace{0.1cm}

        \begin{gather*}
        \mathbf{Variational\ AE}\ \Biggl\{\ \begin{array}{c}
        \text{input} \\
        \mathit{x}
        \end{array} \ \xrightarrow{\text{\textcolor[rgb]{0.82,0.01,0.11}{Encoding}}} \ \begin{array}{c}
        \text{latent distribution} \\
        p(z|x)
        \end{array} \ \xrightarrow{\text{\textcolor[rgb]{0.25,0.46,0.02}{Sampling}}} \ \begin{array}{c}
        \text{sampled latent} \\
        \text{representation} \\
        z \sim p(z|x)
        \end{array} \ \xrightarrow{\text{\textcolor[rgb]{0.29,0.56,0.89}{Decoding}}} \ \begin{array}{c}
        \text{input reconstruction} \\
        d(z)
        \end{array} \ \Biggr\}
        \end{gather*}
        
    
\end{frame}


\begin{frame}{AE’s: Variational Autoencoder (Big Picture)}
    \begin{center}
        \small The key change that they added was that instead of having an encoder that maps points in X to points in Z, VAE’s map points in X to distributions in Z. This allows a point in X to be indirectly mapped to several close neighbour points in Z.

        \includegraphics[width=0.9\textwidth]{pic/VAE big picture.png}

    \end{center}
  
    
\end{frame}



\begin{frame}{AE's: Variational Autoencoder (Details)}
    \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/VAE details1.png}
    \end{center}
    \scriptsize
    \begin{equation*}
        \textbf{VAE loss: } \mathcal{L}(x, \theta, \phi) = 
        \underbrace{
            \| x^{(i)} - \hat{x}^{(i)} \|^2
        }_{\substack{
            \text{\textcolor[rgb]{0.29,0.56,0.89}{reconstruction loss}} \\ 
            \\
            \\
            \downarrow\\
            \\
            f_{\theta}(z) = f_{\theta} \big( \mu_{\phi}(x^{(i)}) + \epsilon \sigma_{\phi}(x^{(i)}) \big)
        }}
        + 
        \underbrace{
            KL\left( q_{\phi}(z | x^{(i)}) \parallel \mathcal{N}(0, I) \right)
        }_{\substack{
            \text{\textcolor[rgb]{0.82,0.01,0.11}{regularization term}} \\ 
            \\
            \downarrow\\
            \\
            \text{Common choice of prior on} \\
            \text{the latent distribution.}
        }}
    \end{equation*}

\end{frame}

\begin{frame}{AE's: Variational Autoencoder (Details)}
    \begin{itemize}
        \item Encourage encodings to be evenly distributed around the center of the latent space
        \item Penalize the network when it tries to memorize data
    \end{itemize}
    
    \begin{center}
        \includegraphics[width=0.8\textwidth]{pic/VAE details2.png}
    \end{center}
\end{frame}

\begin{frame}{AE's: Variational Autoencoder (Details)}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{pic/VAE details3.png}
    \end{center}
    The VAE objectives arranges data on a compact manifold (we can sample from) in a continuous smooth way.

\end{frame}

\begin{frame}{AE’s: Undercomplete Autoencoders}
    \begin{enumerate}
        \item An autoencoder whose code dimension is less than the input dimension is called \textcolor{blue}{undercomplete}.
        \vspace{0.3cm}
        
        \item Learning an undercomplete representation forces the autoencoder to capture the most salient features of the training data.
        \vspace{0.3cm}
        
        \item The learning process is described simply as minimizing a loss function
        \begin{equation*}
            L(x, g(f(x)))
        \end{equation*}
        where \( L \) is a loss function penalizing \( g(f(x)) \) for being dissimilar from \( x \), such as the mean squared error.
    \end{enumerate}
\end{frame}

\begin{frame}{AE’s: Overcomplete Autoencoders}
\small
    \begin{enumerate}
        \item \textcolor{black}{consider encoder} \textit{\textcolor{blue}{f}} \textcolor{black}{and decoder} \textit{\textcolor{blue}{g}}.
        
        \begin{equation*}
            X \in \mathbb{R}^d \quad h \in \mathbb{R}^k
        \end{equation*}
        
        \vspace{0.15cm}
        \item When \( k > d \), the autoencoder is called \textcolor{blue}{Overcomplete Autoencoders}.
        
        \vspace{0.2cm}
        \item There are other ways we can constrain the reconstruction of an autoencoder than to impose a hidden layer of smaller dimension than the input.
        
        \vspace{0.2cm}
        \item Regularized Autoencoders use a loss function that encourages the model to have some properties besides reproducing inputs.
        \begin{itemize}
            \item Sparsity representation (Sparse Autoencoders)
            \item Smallness of derivative of representation (Contractive Autoencoders)
            \item Robustness to noise or to missing inputs (Denoising Autoencoders)
        \end{itemize}
    \end{enumerate}
\end{frame}


\begin{frame}{AE’s: Sparse Autoencoders (Intuition)}
    \scriptsize
    
    \textbf{Idea:} Can we describe the input with a small set of \textit{“attributes”?}
    \\\textbf{This might be a more \textit{compressed} and \textit{structured} representation.}
    
    \begin{columns}[t]
    
        % Left Column
        \begin{column}{0.7\textwidth}
        
             \textbf{\textcolor{deepgreen}{1. NOT structured}} - \textit{“dense”: most values non-zero}
            % Dense Representation Example - Row Layout
            \begin{tcolorbox}[colback=white!10, colframe=white, boxrule=0.5pt, width=\textwidth]
                \begin{columns}[T]
                    \begin{column}{0.2\textwidth}
                        \includegraphics[width=\textwidth]{pic/SAE dog.png} 
                    \end{column}
                    \begin{column}{0.75\textwidth}
                        \scriptsize{\texttt{Pixel (0,0): \#FE057D \\
                        Pixel (0,1): \#FD0263 \\
                        Pixel (0,2): \#E1065F}} \\
                    \end{column}
                \end{columns}
            \end{tcolorbox}

            
        \vspace{0.3cm}

            \textbf{\textcolor{deepred}{Idea: “Sparse” representations are going to be more structured!}}
            
        \vspace{0.6cm}

            % Sparse Representation Example - Row Layout
            \textbf{\textcolor{deepgreen}{2. Very structured!}} - \textit{“sparse”: most values are zero}
            \begin{tcolorbox}[colback=white!10, colframe=white, boxrule=0.5pt, width=0.7\textwidth]
                \begin{columns}[T]
                    \begin{column}{0.3\textwidth}
                        \includegraphics[width=\textwidth]{pic/SAE dog.png} 
                    \end{column}
                    \begin{column}{0.75\textwidth}
                        \scriptsize{\texttt{has\_ears: 1 \\
                        has\_wings: 0 \\
                        has\_wheels: 0}} \\
                    \end{column}
                \end{columns}
            \end{tcolorbox}
            
 
        \end{column}
        
        % Right Column - Aside
        \begin{column}{0.4\textwidth}
        \vspace{-1.3cm}

            \begin{tcolorbox}[colback=blue!5, colframe=blue!70!black, boxrule=0.5pt, width=\textwidth, title=\textbf{Aside:}]
                \scriptsize{This idea originated in neuroscience, where researchers believe that the brain uses \textbf{sparse} representations (see “sparse coding”).}
            \end{tcolorbox}
        \end{column}
        
    \end{columns}
        \vspace{0.2cm}
       \scriptsize{There are many possible “attributes,”\& most images don’t have most of the attributes.}

\end{frame}


\begin{frame}{AE's: Sparese Autoencoder (Big Picture)}
    \begin{center}
        \includegraphics[width=\textwidth]{pic/SAE big picture.png}
    \end{center}

\end{frame}

\begin{frame}{AE’s: Sparse Autoencoders (Details)}
    \vspace{-0.2cm}

    \begin{enumerate}
        \item Sparse Autoencoders try to minimize the following function.
        \[
            L(x, g(f(x))) + \Omega(h)
        \]
        \item The first term is loss for copying inputs.
        \item The second term is sparsity penalty.
        \item In general neural networks, we are trying to find the \textcolor{green!70!black}{\textbf{maximum likelihood:}} \( p(x|\theta) \).
        \item We often use \( \log p(x|\theta) \) for simplification, from which we can get the loss function without regularization.
        \item What about MAP (Maximum a posteriori)?
        \[p(\theta|x) \propto p(x|\theta) \times p(\theta)\]
        \item Maximizing the log of the above function yields:
        \[
            \text{maximize} \ (\log p(x|\theta) + \log p(\theta))
        \]
        \item The first term is the loss function, and the second term is the 
        regularization penalty.
    \end{enumerate}
    \vspace{0.4cm}
\end{frame}


\begin{frame}{AE’s: Denoising Autoencoders (Intuition)}
    \textbf{Idea:} a good model that has learned meaningful structure should “fill in the blanks”
    \hspace{0.3cm}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/DAE intuition.png} 
    \end{center}

    \vspace{0.5cm}
        \begin{tcolorbox}[colback=blue!5, colframe=blue!70!black, boxrule=0.5pt, width=0.9\textwidth, sharp corners=south]
        \textbf{There are \textcolor{blue}{many variants} on this basic idea, and this is one of the most widely used simple autoencoder designs.}
    \end{tcolorbox}

\end{frame}


\begin{frame}{AE’s: Denoising Autoencoders (Details)}
    \small
    \vspace{-0.2cm}

    \begin{enumerate}
        \item \textbf{The denoising autoencoder (DAE)} is an autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its output.
        
        \item \textbf{Traditional autoencoders minimize} {\color{blue} \( L(x, g(f(x))) \)}
        \begin{itemize}
            \item \small \( L \) is a loss function penalizing \( g(f(x)) \) for being dissimilar from \( x \), such as \( L_2 \) norm of difference: mean squared error.
        \end{itemize}

        \item \textbf{A DAE minimizes} {\color{blue} \( L(x, g(f(\tilde{x}))) \)}
        \begin{itemize}
            \item \small \( \tilde{x} \) is a copy of \( x \) that is corrupted by some form of noise.
            \item \small The autoencoder must undo this corruption rather than simply copying their input.
        \end{itemize}
    \end{enumerate}
    
    \vspace{0.3cm}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{pic/DAE details1.png}
    \end{center}
\end{frame}



\begin{frame}{AE’s: Denoising Autoencoders (Training Procedure)}
    \small
    \vspace{-0.3cm}
    \begin{enumerate}
        \item The DAE training procedure is
        \begin{center}
            \includegraphics[width=0.3\textwidth]{pic/DAE training.png} 
        \end{center}
        
        \item We introduce a corruption process \( \textcolor{blue}{C(\tilde{x} | x)} \).
    \end{enumerate}
    \vspace{0.3cm}
\end{frame}

\begin{frame}{AE’s: Denoising Autoencoders (Training Procedure)}
    \small
    \vspace{-0.2cm}
    \begin{enumerate}
        \item We introduce a corruption process \( \textcolor{blue}{C(\tilde{x} | x)} \).
        \item The autoencoder then learns a reconstruction distribution \( \textcolor{blue}{p_{\text{reconstruct}}(x | \tilde{x})} \) estimated from training pairs \( \textcolor{blue}{(x, \tilde{x})} \) as follows:
        \begin{itemize}
            \item Sample a training example \( \textcolor{blue}{x_i} \) from the training data.
            \item Sample a corrupted version \( \textcolor{blue}{\tilde{x}_i} \) from \( \textcolor{blue}{C(\tilde{x}_i | x = x_i)} \).
            \item Use \( \textcolor{blue}{(x, \tilde{x})} \) as a training example for estimating the autoencoder reconstruction distribution \( \textcolor{blue}{p_{\text{reconstruct}}(x|\tilde{x}) = p_{\text{decoder}}(x|h)} \) with \( \textcolor{blue}{h} \) the output of encoder \( \textcolor{blue}{f(\tilde{x})} \) and \( \textcolor{blue}{p_{\text{decoder}}} \) typically defined by a decoder \( \textcolor{blue}{g(h)} \).
            \item Typically we can simply perform gradient-based approximate minimization on the negative log-likelihood \( -\log \textcolor{blue}{p_{\text{decoder}}(x|h)} \).
        \end{itemize}
    \end{enumerate}
    \vspace{0.3cm}
\end{frame}


\begin{frame}{AE’s: Denoising Autoencoders (Result On MNIST)}
      \begin{center}
        \includegraphics[width=\textwidth]{pic/DAE result on MNIST.png} 
    \end{center}
\end{frame}

\begin{frame}{AE’s: Denoising Autoencoders (It Isn’t Lazy Network!)}
    \begin{itemize}
        \item DAEs won’t simply memorize the inputs and outputs (More robustness)
        \item Intuitively, a DAE learns a \textcolor{blue}{projection} from a neighborhood of our training data back onto the training data 
    \end{itemize}

    \begin{center}
        \includegraphics[width=0.7\textwidth]{pic/DAE not lazy1.png}
    \end{center}
\end{frame}

\begin{frame}{AE’s: Denoising Autoencoders (It Isn’t Lazy Network!)}
    \begin{itemize}
        \item DAEs won’t simply memorize the inputs and outputs (More robustness)
        \item Intuitively, a DAE learns a \textcolor{blue}{projection} from a neighborhood of our training data back onto the training data 
    \end{itemize}

    \begin{center}
        \includegraphics[width=0.65\textwidth]{pic/DAE not lazy2.png}
    \end{center}
\end{frame}

\begin{frame}{AE’s: Denoising Autoencoders (It Isn’t Lazy Network!)}
    \begin{itemize}
        \item  DAEs won’t simply memorize the inputs and outputs (More robustness)
        \item Intuitively, a DAE learns a \textcolor{blue}{projection} from a neighborhood of our training data back onto the training data 
    \end{itemize}

    \begin{center}
        \includegraphics[width=0.6\textwidth]{pic/DAE not lazy3.png}
    \end{center}
\end{frame}

\begin{frame}{AE’s: Contractive Autoencoder (Intuition)}
    \small
    \begin{enumerate}
        \item Contractive autoencoders are explicitly encouraged to learn a manifold through their loss function (Alain and Bengio \textcolor{blue}{2014}).
        \item \textcolor{deepgreen}{Desirable property:} Points close to each other in input space maintain that property in the latent space.
        \item Method to avoid uninteresting solutions
        \item Add an explicit term in the loss that penalizes that solution
        \item We wish to extract features that only reflect variations observed in the training set
        \item We would like to be invariant to other variations
    \end{enumerate}
    
    
    \begin{center}
        \includegraphics[width=0.45\textwidth]{pic/CAE Intuition1.png} 
    \end{center}
    \vspace{0.4cm}

\end{frame}


\begin{frame}{AE’s: Contractive Autoencoder (Intuition)}
      \begin{center}
        \includegraphics[width=0.7\textwidth]{pic/CAE Intuition2.png} 
    \end{center}
\end{frame}


\begin{frame}{AE’s: Contractive Autoencoder (Details)}
    \scriptsize
    \begin{enumerate}
        \item Contractive autoencoder has an explicit regularizer on \( h = f(x) \), encouraging the derivatives of \( f \) to be as small as possible.
        \item This will be true if \( f(x) = h \) is continuous, has small derivatives.
    \end{enumerate}

    \begin{center}
        \includegraphics[width=0.2\textwidth]{pic/CAE details1.png}
    \end{center}

    \begin{enumerate}
        \setcounter{enumi}{2} % Continue numbering from 3
        \item We can use the \textcolor{blue}{Frobenius Norm} of the \textcolor{blue}{Jacobian Matrix} as a regularization term:
        
        \[
        \Omega(f, x) = \lambda \left\| \frac{\partial f(x)}{\partial x} \right\|^2_F
        \]
        
        \item These autoencoders are called \textit{contractive} because they contract the neighborhood of input space into a smaller, localized group in latent space.
        
        \item \textbf{\textcolor{red}{Exercise:}} What is the difference between DAE and CAE?
    \end{enumerate}
    
    \vspace{0.4cm}
\end{frame}


\begin{frame}{AE’s: Contractive Autoencoder (Details)}
    \scriptsize

    \begin{itemize}
        \item New loss function:
    \end{itemize}
    
 
        \begin{equation*}
        \ \underbrace{l\left( f\left( x^{( t)}\right)\right)}_{ \begin{array}{l}
        \mathtt{autoencoder\ }\\
        \mathtt{reconstruction}
        \end{array}} +\ \lambda \ \underbrace{\| \nabla _{x^{( t)}} h( x^{( t)} \| _{F}^{2}}_{ \begin{array}{l}
        \mathtt{Jacobian\ of\ }\\
        \mathtt{encoder}
        \end{array}}
        \end{equation*}
        
    \begin{itemize}
        \item where, for binary observations:
    \end{itemize}

            
        \begin{gather*}
        \begin{array}{ c|}
        l\left( f\left( x^{( t)}\right)\right) \ =\ -\Sigma _{k} \ ( x_{k}^{( t)} \ \log\left(\hat{x}_{k}^{( t)}\right) \ +\ \left( 1-x_{k}^{( t)}\right)\log\left( 1-\hat{x}_{k}^{( t)}\right) \ \ \ \Biggr\}\begin{array}{ c }
        \mathtt{encoders\ keeps}\\
        \mathtt{good\ information}
        \end{array}\\
         \\\\
        \| \nabla _{x^{( t)}} h( x^{( t)} \| _{F}^{2} \ =\ \Sigma _{j} \Sigma _{k}\left(\frac{\partial h\left( x^{( t)}\right)_{j}}{\partial x_{k}^{( t)}}\right)^{2} \ \ \ \Biggr\}\begin{array}{ c }
        \mathtt{encoder\ throws}\\
        \mathtt{away\ all\ information}
        \end{array}
        \end{array}\rightarrow \mathtt{\begin{array}{ c }
        \mathtt{encoders\ keep\ only\ }\\
        \mathtt{good\ information}
        \end{array}}\\
        \end{gather*}
\end{frame}


\begin{frame}{AE other Applications: Anomaly Detection}
    \begin{columns}
        % Left Image
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{pic/AE other app anomaly detection1.png}
            \end{center}
        \end{column}

        % Right Image
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{pic/AE other app anomaly detection2 .png}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{AE other Applications: Image Colorization}
      \begin{center}
        \includegraphics[width=0.7\textwidth]{pic/AE other app colorization.png} 
    \end{center}
\end{frame}


\begin{frame}{AE other Applications: Fake Lough}
      \begin{center}
        \includegraphics[width=0.8\textwidth]{pic/AE other app Fake Lough.png} 
    \end{center}
\end{frame}



\section{Segmentation architectures}

\subsection{Task Introduction}

\begin{frame}{Problem Setup}
    \vspace{-0.1cm}

    \begin{columns}[t] % Use top alignment for better layout control
        % Left column with "Before..."
        \begin{column}{0.5\textwidth}
            \textbf{Before...} % Remove center alignment for left-alignment
            \vspace{0.2cm}
            \includegraphics[width=\textwidth]{pic/SS problem setup before.png}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            \textbf{Now...}
            \vspace{0.2cm}
            \includegraphics[width=\textwidth]{pic/SS problem setup now1.png} 
        \end{column}
    \end{columns}
    
     \textbf{\textcolor{red}{Problem:}} \textcolor{red}{We want the output to have the same resolution as the input!}
    % Text below the "Now..." section
    \begin{itemize}
        \item Label every single pixel with its class. Actually simpler in some sense:
        \begin{itemize}
            \item[] \textbullet \ No longer variable \# of outputs. \hspace{1em}\textbullet \ Every pixel has a label.
        \end{itemize}
    \end{itemize}

\end{frame}


\begin{frame}{Problem Setup}
    \begin{center}
        \includegraphics[width=0.5\textwidth]{pic/SS problem setup now2.png}
    \end{center}
    
    
    \begin{minipage}{0.9\textwidth}\footnotesize
        \textbf{Classify every point with a class} \\
        Don't worry for now about instances (e.g., two adjacent cows are just one "cow blob" and that’s OK for some reason)\\

                
        \textbf{The challenge: design a network architecture} \\
        that makes this \textbf{"per-pixel classification"} problem  \color{deepblue}{computationally tractable}
    \end{minipage}
\end{frame}


\begin{frame}{Semantic Segmentation: Sliding Window}
    \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/SS sliding window.png}
    \end{center}
    
    \vspace{0.3cm}
    
    \textcolor{red}{\textbf{Problem: Very inefficient! Not reusing shared features between overlapping patches}}
    
    \vspace{0.3cm}
    
    \scriptsize{
    Farabet et al., “Learning Hierarchical Features for Scene Labeling,” TPAMI 2013 \\
    Pinheiro and Collobert, “Recurrent Convolutional Neural Networks for Scene Labeling,” ICML 2014
    }
    \vspace{0.2cm}

\end{frame}

\begin{frame}{Semantic Segmentation Idea: Fully Convolutional}
  
    
    \begin{center}
        Design a network as a bunch of convolutional layers \\
        to make predictions for pixels all at once!
    \end{center}

      \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/SS fully conv1.png}
    \end{center}
    
    \vspace{0.1cm}
    
    
    \textcolor{red}{\textbf{Problem: convolutions at original image resolution will be very expensive ...}}
\end{frame}

\begin{frame}{Semantic Segmentation Idea: Fully Convolutional}
    \textcolor{red}{\textbf{So far we learn about downsampling...}}

    \begin{center}
        \vspace{0.3cm}
        Design network as a bunch of convolutional layers, \\
        with \textcolor{red}{downsampling} and \textcolor{blue}{upsampling} inside the network!
    \end{center}
    
    \begin{center}
        \includegraphics[width=0.9\textwidth]{pic/SS fully conv2.png}
    \end{center}
    
    \vspace{0.5cm}
    
    \scriptsize{
    Long, Shelhamer, and Darrell, “Fully Convolutional Networks for Semantic Segmentation,” CVPR 2015 \\
    Noh et al., “Learning Deconvolution Network for Semantic Segmentation,” ICCV 2015
    }
\end{frame}

\begin{frame}{Recap: Types of Downsampling}
    \begin{itemize}
        \item Max Pooling
        \item Average Pooling
        \item Global Average Pooling
        \item Strided Convolution
        \item Spatial Pyramid Pooling
        \item \ldots
    \end{itemize}
\end{frame}

\begin{frame}{Details of Downsampling Methods}
    \scriptsize
    \begin{table}[]
        \centering
        \renewcommand{\arraystretch}{1.2} % Slightly increase row height for readability
        \begin{tabular}{|p{2.5cm}|p{3.5cm}|p{2.8cm}|p{3.5cm}|}
            \hline
            \textbf{Pooling Method} & \textbf{Key Features} & \textbf{Best Applications} & \textbf{Rule of Thumb} \\ \hline
            Max Pooling & Captures the most important feature in a region (e.g., edges) & Image classification, object detection & Use when sharp features (e.g., edges) are essential. \\ \hline
            Average Pooling & Smooths the feature map, averages over a region & Smoothing features, object detection & Use for smoother output, where fine detail is less important. \\ \hline
            Global Average Pooling (GAP) & Replaces fully connected layers, outputs a single value per channel & Final layer in classification networks (e.g., ResNet, Inception) & Ideal for reducing model complexity and overfitting. \\ \hline
            Strided Convolution & Combines downsampling and feature extraction & Object detection, fully convolutional networks (FCNs) & Use for efficiency when feature extraction and downsampling can be combined. \\ \hline
            Spatial Pyramid Pooling (SPP) & Captures multi-scale features with pooling at multiple scales & Multi-scale object detection, semantic segmentation & Use when recognizing objects at various scales is critical. \\ \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{Fully Convolutional Networks (FCN)}
 
    \begin{center}
      Design network as a bunch of convolutional layers,
        \\ with \textcolor{red}{downsampling} and \textcolor{blue}{upsampling} inside the network!
        \vspace{0.2cm}

        \includegraphics[width=0.9\textwidth]{pic/SS fully conv3.png}
    \end{center}

\end{frame}


\subsection{Transposed Convolution}

\begin{frame}{Up Samplings}
    \begin{itemize}
        \item[] We can divide methods into:
        \vspace{0.3cm}
        \item Static up sampling's
        \item Learnable up sampling's
    \end{itemize}
\end{frame}


\begin{frame}{Static Upsampling: ``Unpooling''}

\begin{columns}[c]
    % Left Column
    \begin{column}{0.5\textwidth}
        \centering
        \textbf{"Nearest Neighbor"}\\
        \vspace{0.3cm}
        \begin{tabular}{|c|c|}
            \hline
            1 & 2 \\ \hline
            3 & 4 \\ \hline
        \end{tabular}
        \hspace{0.3cm} $\longrightarrow$ \hspace{0.3cm}
        \begin{tabular}{|c|c|c|c|}
            \hline
            1 & 1 & 2 & 2 \\ \hline
            1 & 1 & 2 & 2 \\ \hline
            3 & 3 & 4 & 4 \\ \hline
            3 & 3 & 4 & 4 \\ \hline
        \end{tabular}
        \vspace{0.3cm}\\
          \begin{flushleft}
        \text{ \hspace{0.4cm} Input: 2 x 2 \hspace{1.2cm} Output: 4 x 4}
        \end{flushleft}
    \end{column}

    % Right Column
    \begin{column}{0.5\textwidth}
        \centering
        \textbf{``Bed of Nails''}\\
        \vspace{0.3cm}
        \begin{tabular}{|c|c|}
            \hline
            1 & 2 \\ \hline
            3 & 4 \\ \hline
        \end{tabular}
        \hspace{0.3cm} $\longrightarrow$ \hspace{0.3cm}
        \begin{tabular}{|c|c|c|c|}
            \hline
            1 & 0 & 2 & 0 \\ \hline
            0 & 0 & 0 & 0 \\ \hline
            3 & 0 & 4 & 0 \\ \hline
            0 & 0 & 0 & 0 \\ \hline
        \end{tabular}
        \vspace{0.3cm}\\
        \begin{flushleft}
        \text{ \hspace{0.4cm} Input: 2 x 2 \hspace{1.2cm} Output: 4 x 4}
        \end{flushleft}
    \end{column}
\end{columns}

\end{frame}



\begin{frame}{Static Upsampling: ``Max Unpooling''}
 
    \begin{center}
        \includegraphics[width=\textwidth]{pic/Max Unpooling.png}
    \end{center}

\end{frame}


\begin{frame}{Static Upsamplings}
 
    \begin{center}
        \includegraphics[width=0.75\textwidth]{pic/Static Upsampling.png}
    \end{center}

\end{frame}

\begin{frame}{Convolution and Transposed Convolution}
\small
\vspace{-0.3cm}

% Main content in two columns
\begin{columns}[T]

    % Left Column for Normal Convolution
    \begin{column}{0.5\textwidth}
        \textbf{Normal Convolutions:} reduce resolution with \textbf{stride}\\
        \begin{center}
            \textbf{Stride = 2}\\
            \includegraphics[width=0.8\textwidth]{pic/Conv vs Transpose Conv1.png}\\ 
            \vspace{0.75cm}
            \textbf{Input:} \( H_f \times W_f \times C_{in} \) \\
            \textbf{Output:} \( 1 \times 1 \times C_{out} \) \\
            \textbf{Filter:} \( H_f \times W_f \times C_{in} \times C_{out} \)
        \end{center}
    \end{column}

    % Right Column for Transposed Convolution
    \begin{column}{0.5\textwidth}
        \textbf{Transposed Convolutions:} increase resolution with \textbf{fractional ``stride''}\\
        \begin{center}
            \textbf{Stride = 1/2}\\
            \includegraphics[width=0.85\textwidth]{pic/Conv vs Transpose Conv2.png}\\ 
              \textcolor{deepblue}{We have two sets of values here,} \textbf{\textcolor{deepblue}{sum up them!}}\\
            \vspace{0.2cm}
            \textbf{Input:} \( 1 \times 1 \times C_{in} \) \\
            \textbf{Output:} \( H_f \times W_f \times C_{out} \) \\
            \textbf{Filter:} \( C_{in} \times H_f \times W_f \times C_{out} \)
        \end{center}
    \end{column}

\end{columns}

\end{frame}


\begin{frame}{1D Convolution: A Matrix Perspective}
\small
\begin{itemize}
    \item Display the convolution matrix with a 1D kernel \([1, 1, 1]\) sliding over an input vector.
\end{itemize}

\vspace{0.3cm}

\[
\begin{bmatrix}
    1 & 1 & 1 & 0 & \dots & 0 \\
    0 & 1 & 1 & 1 & \dots & 0 \\
    0 & 0 & 1 & 1 & 1 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & \dots & 0 & 1 & 1 & 1
\end{bmatrix}
\begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3 \\
    \vdots \\
    x_n
\end{bmatrix}
\]

\vspace{0.5cm}

\begin{itemize}
    \item This representation mimics a sliding window operation where the kernel moves across the input, performing element-wise multiplications and summing up the results.
\end{itemize}

\end{frame}


\begin{frame}{1D Transposed Convolution: A Matrix Perspective}
\small
\begin{itemize}
    \item The operation is equivalent to taking the transpose of the kernel matrix and applying it to the input:
\end{itemize}

\[
\begin{bmatrix}
    1 & 0 & 0 & 0 & \cdots & 0 \\
    1 & 1 & 0 & 0 & \cdots & 0 \\
    1 & 1 & 1 & 0 & \cdots & 0 \\
    \cdots & 1 & 1 & 1 & 0 & 0 \\
    \cdots & \cdots & 1 & 1 & 1 & 0 \\
    \cdots & \cdots & \cdots & 1 & 1 & 1 \\
    0 & 0 & 0 & \cdots & 1 & 1
\end{bmatrix}
\begin{bmatrix}
    x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_k
\end{bmatrix}
\quad = \quad
\begin{bmatrix}
    A_1 & A_2 & \cdots & A_k
\end{bmatrix}
\begin{bmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_k
\end{bmatrix}
= \sum_{i=1}^k x_i A_i
\]


\begin{columns}[T]
    \begin{column}{0.75\textwidth}
      \vspace{0.3cm}
        \begin{itemize}
            \item Since the transposed convolution uses the \textbf{transpose} of the kernel matrix and effectively reverses the compression process (convolution), it’s called \textbf{"Transposed Convolution"}.
        \end{itemize}
    \end{column}

  \begin{column}{0.34\textwidth}
    \scriptsize
    \textbf{Other names:}
    \vspace{-0.2cm}
    \begin{itemize}
        \setlength{\itemsep}{0pt} % Adjust the space between items
        \setlength{\parskip}{0pt} % Adjust the paragraph spacing
        \item Deconvolution (bad name)
        \item Upconvolution
        \item Fractionally strided convolution
        \item Backward strided convolution
    \end{itemize}
\end{column}

\end{columns}

\end{frame}


\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg1.png} 
    \end{center}
\end{frame}


\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg2.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg3.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg4.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg5.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg6.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg7.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg8.png} 
    \end{center}
\end{frame}

\begin{frame}{Learnable Upsampling: “Transpose Convolution”}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Transpose conv eg9.png} 
    \end{center}
\end{frame}



\begin{frame}{Transpose Convolution: 1D Example}
    \begin{columns}[T]
        % Left column for Input, Filter, and Output diagram
        \begin{column}{0.75\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/Transpose Conv 1d example.png}
        \end{column}

        % Right column for explanation text
        \begin{column}{0.35\textwidth}
            \small
            \vspace{1cm}
            \begin{itemize}
                \item \textbf{Output} contains copies of the filter weighted by the \textbf{input}, summing at overlaps in the output.
                \item \textbf{Need to crop} one pixel from the output to make it exactly \textbf{2x input}.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}{Transpose Convolution: 2D Example}

    \begin{center}
        \includegraphics[width=1.05\textwidth]{pic/Transpose Conv 2d example.png} 
    \end{center}

    \vspace{0.6cm}
    \tiny{Image from \href{https://mriquestions.com/upsampling.html}{this link}}
\end{frame}


\subsection{Case Study: U-Net}


\begin{frame}{U-Net: Purpose \& Origin}
    \begin{itemize}
        \item Developed by Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
        \item First released in 2015, presented at the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI).
        \item Designed specifically for biomedical \textcolor{deepblue}{image segmentation} tasks.
    \end{itemize}
    \begin{center}
        \includegraphics[width=\textwidth]{pic/Unet origin.png} 
    \end{center}

    \vspace{0.3cm}
    \tiny{Image from \href{https://miro.medium.com/v2/resize:fit:1078/1*Dpmg8bywcabPdX08BtlleQ.png}{this link}}
\end{frame}


\begin{frame}{U-Net: Usage Over Time}
    \begin{itemize}
        \item The rise in its usage is due to its simplicity, effectiveness, and adaptability to various image processing tasks beyond medical imaging.
    \end{itemize}

    \begin{center}
        \includegraphics[width=\textwidth]{pic/Unet usage.png} 
    \end{center}

    \vspace{1.4cm}
    \tiny{From \href{https://paperswithcode.com/method/u-net}{papers with code}}
\end{frame}



\begin{frame}{U-Net: Tasks}

    \begin{center}
        \includegraphics[width=1.05\textwidth]{pic/Unet Tasks.png} 
    \end{center}

    \vspace{0.6cm}
    \tiny{From \href{https://paperswithcode.com/method/u-net}{papers with code}}
\end{frame}


\begin{frame}{U-Net: Big Picture}

    \begin{center}
        \includegraphics[width=0.95\textwidth]{pic/Unet big pic1.png} 
    \end{center}
    \tiny{From \href{https://arxiv.org/abs/1505.04597}{Unet}}
\end{frame}


\begin{frame}{U-Net: Skip Connections}

    \begin{center}
        \includegraphics[width=0.75\textwidth]{pic/Unet-Skip connections.png} 
    \end{center}
    \tiny{From \href{https://arxiv.org/abs/1505.04597}{Unet}}
\end{frame}


\begin{frame}{U-Net vs AE's: The Skip Connection's}
\small
\textbf{Preserving Spatial Features \& Reducing Information Bottlenecks:}
\begin{itemize}
    \item Without skip connections, deep layers compress data heavily, leading to loss of spatial details, blurry outputs, and poor segmentation quality, especially in fine edges and boundaries.
    \item Skip connections bypass the bottleneck by passing high-resolution spatial details directly from encoder to decoder, helping retain sharpness and segmentation accuracy.
\end{itemize}

\vspace{0.5cm}

\textbf{Improving Network Stability and Learning:}
\begin{itemize}
    \item Skip connections allow gradients to flow more effectively during backpropagation, mitigating the vanishing gradient problem common in deep networks.
\end{itemize}

\end{frame}



\begin{frame}{U-Net vs AE's: Latent Space}
\small

\textcolor{red}{\textbf{Unlike AE’s (specially VAE), U-Net’s latent space is less expressive:}}

\begin{itemize}
    \item \textbf{Skip Connections Reduce the Bottleneck's Importance:} Because of skip connections, U-Net doesn’t rely solely on the latent space for feature representation. Thus, the latent space is not required to encode as much high-frequency information as in an AE. As a result, its representation isn’t as structured or “clustered,” meaning a t-SNE plot of U-Net’s latent space would not show the same level of clustering we see with VAEs, where every detail must be inferred from the bottleneck.
    
    \vspace{0.3cm}
    
    \item \textbf{No Strong Encoding Constraint:} Since skip connections provide spatial detail, U-Net’s latent space is not forced to develop an “expressive” representation, which is essential for models like VAEs where information passes strictly through a bottleneck.
\end{itemize}

\end{frame}



\begin{frame}{U-Net: Big Picture}

    \begin{center}
        \includegraphics[width=0.95\textwidth]{pic/Unet big picture3.png} 
    \end{center}
    \tiny{From \href{https://www.youtube.com/watch?v=NhdzGfB1q74}{this link}}
\end{frame}



\begin{frame}{U-Net: Big Picture}

    \begin{center}
        \includegraphics[width=0.85\textwidth]{pic/Unet big picture4.png} 
    \end{center}
    \tiny{From \href{https://arxiv.org/abs/1505.04597}{Unet}}
\end{frame}



\begin{frame}{U-Net: Big Picture}

    \begin{center}
        \includegraphics[width=0.95\textwidth]{pic/Unet big picture5.png} 
    \end{center}
    \tiny{From \href{https://www.youtube.com/watch?v=NhdzGfB1q74}{this link}}
\end{frame}



\begin{frame}{U-Net: Encoder}

    \begin{center}
        \includegraphics[width=0.7\textwidth]{pic/Unet-Encoder1.png} 
    \end{center}
\end{frame}

\begin{frame}{U-Net: Encoder}
    \begin{columns}[T] % Split the frame into two vertical columns
    
        % Left Column
        \begin{column}{0.7\textwidth}
            \begin{itemize}
            \scriptsize
                \item Repeated 3x3 convolutional with \textbf{\textcolor{red!60!black}{valid padding}} (= no padding) + ReLU layers
                \item 2x2 max pooling layers to down sample
                \item \textbf{\textcolor{yellow!60!black}{Double channels}} with conv after the \textbf{\textcolor{green!60!black}{max pool}}
                \item Due to the \textbf{\textcolor{blue!60!black}{shrinking effect}} of valid convs, \textbf{\textcolor{cyan!60!black}{cropped part}} is concatenated with upsampled feature map
            \end{itemize}
            % Shift figure slightly to the right
            \begin{center}
                \hspace{0.5cm} % Adjust horizontal spacing
                \includegraphics[width=0.9\textwidth]{pic/Unet-Encoder2.png}
            \end{center}
        \end{column}
        
        % Right Column
        \begin{column}{0.2\textwidth}
            % Place Figure 3
            \begin{center}
                \includegraphics[width=0.7\textwidth]{pic/Unet-Encoder3.png}
                \vspace{3cm}
            \end{center}
        \end{column}
        
    \end{columns}
\end{frame}


\begin{frame}{U-Net: Bottleneck}
    \begin{itemize}
    \small
    \setlength\itemsep{1em} 
        \item Down sample with \textbf{2x2 max pooling}
        \item Repeated \textbf{3x3 convolutional} with \textcolor{red!60!black}{valid pooling} (= no padding) + ReLU layers
        \item \textbf{Double channels} with conv after the max pool
    \end{itemize}

    \begin{center}
        \includegraphics[width=0.5\textwidth]{pic/Unet Bottleneck.png}
    \end{center}
\end{frame}


\begin{frame}{U-Net: Decoder}
    \scriptsize
    \begin{columns} % Use two-column layout
        \begin{column}{0.7\textwidth}
        \vspace{-1cm}
            \begin{itemize}
                \setlength\itemsep{1em} % Adjust spacing between bullet points
                \item Repeated 3x3 convolutional with \textbf{valid pooling} (= no padding) + ReLU layers
                \item Upsampling, followed by 2x2 convolutional layer
                \item Halve channels after upsampling convolution
            \end{itemize}
            
        \end{column}

        % Right column for the figure
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[width=0.6\textwidth]{pic/Unet decoder.png} 
            \end{center}
        \end{column}
    \end{columns}
\end{frame}



\begin{frame}{U-Net: Skip Connections}
\small
    \textbf{Skip Connections:}
    \begin{itemize}
        \item Connect encoder layers to decoder layers at the same depth.
        \item Preserve high-resolution features for better upsampling.
    \end{itemize}
    
    \vspace{0.5cm} % Add vertical space between sections
    
    \textbf{Cropping for Alignment:}
    \begin{itemize}
        \item Due to valid padding, feature maps in the encoder are larger than those in the decoder.
        \item Cropping aligns sizes before concatenation in skip connections.
    \end{itemize}
    
    \vspace{0.5cm} % Add vertical space between sections
    
    \textbf{Why Important?:}
    \begin{itemize}
        \item Combines local details (from encoder) with context (from decoder).
        \item Helps in precise pixel-wise segmentation.
        \item Overcomes vanishing gradient issue by allowing smooth information flow.
    \end{itemize}
\end{frame}





\begin{frame}{Data Efficiency in U-Net}
\scriptsize
\begin{itemize}
    \item \textbf{Residual and Skip Connections:} These connections reduce the amount of information the model needs to learn, as many spatial details are preserved, requiring less adaptation from the model. The need to learn detailed spatial arrangements is mitigated by direct concatenation, which efficiently reintroduces spatial information.
    \vspace{0.3cm}
    \item \textbf{Fully Convolutional Nature:} U-Net uses only convolutional layers, which have fewer parameters compared to fully connected networks. This allows the model to generalize well from smaller datasets.
    \vspace{0.3cm}
    \item \textbf{Data Augmentation Advantages:} Because U-Net is often used for pixel-wise tasks like semantic segmentation, every augmentation (rotation, scaling, etc.) maintains a one-to-one correspondence with ground truth masks. This allows each augmentation to act as a ``new'' data point, further enriching the dataset without additional manual labeling.
    \vspace{0.3cm}
    \item \textbf{Patch-Based Training:} Instead of processing the entire image (e.g., a 1024x1024 image), U-Net can work with smaller patches, increasing batch sizes and enabling more efficient use of each image during training.
\end{itemize}
\end{frame}





\section{References}

\begin{frame}[allowframebreaks]
    \bibliography{ref}
    \bibliographystyle{ieeetr}
    \nocite{*} % used here because no citation happens in slides
    \small
    \begin{itemize}
        \item F.-F. Li, J. Wu, and R. Gao, “CS231n” Lecture slides, 2024, Stanford 
        \item A. Amini, “6s191” Lecture slides, 2024, MIT. 
        \item M. Soleymani, “Deep Learning” Lecture slides, 2024, Sharif University of Technology.
        \item H. Beigy, “Deep Learning” Lecture slides, 2022, Sharif University of Technology.
        \item S. Levine, “CS W182/282A” Lecture slides, 2024, UC Berkeley 
        \item Y. Maziane, “Diffusion models: Seek of information and structure in latent space,” 2022, University of Liège.
        \item G. Buzzard, “Mathematical Aspects of Neural Networks” Lecture slides, 2019, Purdue University.
    \end{itemize}
\end{frame}


\begin{frame}
    \begin{center}
        {\Huge Any Questions?}
    \end{center}
\end{frame}

\end{document}